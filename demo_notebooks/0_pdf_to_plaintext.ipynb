{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 0: PDF to Plain Text Extraction\n",
    "\n",
    "This notebook extracts plain text from PDF files using [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/), with automatic fallback to OCR when needed.\n",
    "\n",
    "**Handles both text-based and image-based (scanned) PDFs with a 3-tier extraction strategy:**\n",
    "1. **PyMuPDF `get_text()`** — fast, works for text-based PDFs\n",
    "2. **Tesseract OCR** — handles scanned/image-based pages where PyMuPDF returns no text\n",
    "3. **Docling** — last-resort fallback for complex layouts, tables, or formats that defeat Tesseract\n",
    "\n",
    "The notebook auto-detects which method is needed per-page.\n",
    "\n",
    "## Use Case\n",
    "If your source documents are PDFs (e.g. corporate filings, legal documents, reports), this notebook converts them to plain text files that can then be fed into the import pipeline in **Notebook 1**.\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- **Tesseract OCR** installed on your system (`brew install tesseract` on macOS, `apt install tesseract-ocr` on Linux)\n",
    "- PDF files to process (place them in a directory of your choice)\n",
    "\n",
    "### Expected Runtime\n",
    "- Text-based PDFs: ~1-2 seconds per page\n",
    "- OCR (Tesseract): ~2-5 seconds per page\n",
    "- Docling fallback: ~5-10 seconds per page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (1.26.7)\n",
      "Requirement already satisfied: tqdm in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting docling\n",
      "  Downloading docling-2.75.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling) (2.12.5)\n",
      "Collecting docling-core<3.0.0,>=2.62.0 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading docling_core-2.66.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting docling-parse<6.0.0,>=5.3.2 (from docling)\n",
      "  Downloading docling_parse-5.4.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
      "  Downloading docling_ibm_models-3.11.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pypdfium2!=4.30.1,<6.0.0,>=4.30.0 (from docling)\n",
      "  Downloading pypdfium2-5.5.0-py3-none-macosx_11_0_arm64.whl.metadata (68 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling)\n",
      "  Downloading pydantic_settings-2.13.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling) (0.36.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling) (2.32.5)\n",
      "Collecting ocrmac<2.0.0,>=1.0.0 (from docling)\n",
      "  Downloading ocrmac-1.0.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
      "  Downloading rapidocr-3.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling) (2026.1.4)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
      "  Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting typer<0.22.0,>=0.12.5 (from docling)\n",
      "  Downloading typer-0.21.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from docling)\n",
      "  Using cached beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling) (2.3.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
      "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lxml<7.0.0,>=4.0.0 (from docling)\n",
      "  Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pillow<13.0.0,>=10.0.0 (from docling)\n",
      "  Using cached pillow-12.1.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pluggy<2.0.0,>=1.0.0 (from docling)\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy<2.0.0,>=1.6.0 (from docling)\n",
      "  Using cached scipy-1.17.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting accelerate<2,>=1.0.0 (from docling)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting polyfactory>=2.22.2 (from docling)\n",
      "  Downloading polyfactory-3.3.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from docling)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (2.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4<5.0.0,>=4.12.3->docling)\n",
      "  Using cached soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
      "Collecting jsonschema<5.0.0,>=4.16.0 (from docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Using cached jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling) (0.9.0)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tree-sitter<0.27.0,>=0.25.0 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading tree_sitter-0.25.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting tree-sitter-python>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting tree-sitter-c>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting tree-sitter-javascript>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting tree-sitter-typescript>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from docling-core[chunking]<3.0.0,>=2.62.0->docling) (4.57.6)\n",
      "Collecting torchvision<1,>=0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
      "  Downloading torchvision-0.25.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from huggingface_hub<1,>=0.23->docling) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from huggingface_hub<1,>=0.23->docling) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
      "Collecting attrs>=19.2.0 (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.62.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Using cached rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Click>=7.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from ocrmac<2.0.0,>=1.0.0->docling) (8.3.1)\n",
      "Collecting pyobjc-framework-Vision (from ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_vision-12.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading pyclipper-1.4.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (8.6 kB)\n",
      "Collecting opencv_python>=4.5.1.48 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading opencv_python-4.13.0.92-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
      "Collecting Shapely!=2.0.4,>=1.7.1 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting omegaconf (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.2->docling) (2.6.3)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
      "Collecting torch>=2.0.0 (from accelerate<2,>=1.0.0->docling)\n",
      "  Downloading torch-2.10.0-2-cp313-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.62.0->docling) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.62.0->docling) (0.22.2)\n",
      "Collecting annotated-doc>=0.0.2 (from typer<0.22.0,>=0.12.5->docling)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.22.0,>=0.12.5->docling)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<0.22.0,>=0.12.5->docling)\n",
      "  Downloading rich-14.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
      "  Downloading faker-40.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<0.22.0,>=0.12.5->docling)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<0.22.0,>=0.12.5->docling) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.22.0,>=0.12.5->docling)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
      "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading multiprocess-0.70.19-py313-none-any.whl.metadata (7.5 kB)\n",
      "Collecting dill>=0.4.1 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.62.0->docling)\n",
      "  Downloading dill-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc-core>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_core-12.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (2.8 kB)\n",
      "Collecting pyobjc-framework-Cocoa>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_cocoa-12.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-Quartz>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_quartz-12.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pyobjc-framework-CoreML>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_coreml-12.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (2.5 kB)\n",
      "Downloading docling-2.75.0-py3-none-any.whl (396 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Using cached beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading docling_core-2.66.0-py3-none-any.whl (241 kB)\n",
      "Downloading docling_ibm_models-3.11.0-py3-none-any.whl (87 kB)\n",
      "Downloading docling_parse-5.4.0-cp313-cp313-macosx_14_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
      "Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl (8.6 MB)\n",
      "Downloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
      "Downloading ocrmac-1.0.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached pillow-12.1.1-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic_settings-2.13.1-py3-none-any.whl (58 kB)\n",
      "Downloading pypdfium2-5.5.0-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading rapidocr-3.6.0-py3-none-any.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "Using cached scipy-1.17.1-cp313-cp313-macosx_14_0_arm64.whl (20.3 MB)\n",
      "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading torchvision-0.25.0-cp313-cp313-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.10.0-2-cp313-none-macosx_11_0_arm64.whl (79.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter-0.25.2-cp313-cp313-macosx_11_0_arm64.whl (137 kB)\n",
      "Downloading typer-0.21.2-py3-none-any.whl (56 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading opencv_python-4.13.0.92-cp37-abi3-macosx_13_0_arm64.whl (46.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading polyfactory-3.3.0-py3-none-any.whl (62 kB)\n",
      "Downloading faker-40.5.1-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.4.0-cp313-cp313-macosx_10_13_universal2.whl (264 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rich-14.3.3-py3-none-any.whl (310 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached rpds_py-0.30.0-cp313-cp313-macosx_11_0_arm64.whl (358 kB)\n",
      "Downloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached soupsieve-2.8.3-py3-none-any.whl (37 kB)\n",
      "Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl (86 kB)\n",
      "Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (66 kB)\n",
      "Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (76 kB)\n",
      "Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl (302 kB)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Downloading multiprocess-0.70.19-py313-none-any.whl (156 kB)\n",
      "Downloading dill-0.4.1-py3-none-any.whl (120 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pyobjc_framework_vision-12.1-cp313-cp313-macosx_10_13_universal2.whl (16 kB)\n",
      "Downloading pyobjc_core-12.1-cp313-cp313-macosx_10_13_universal2.whl (677 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.4/677.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_cocoa-12.1-cp313-cp313-macosx_10_13_universal2.whl (384 kB)\n",
      "Downloading pyobjc_framework_coreml-12.1-cp313-cp313-macosx_10_13_universal2.whl (11 kB)\n",
      "Downloading pyobjc_framework_quartz-12.1-cp313-cp313-macosx_10_13_universal2.whl (219 kB)\n",
      "Building wheels for collected packages: pylatexenc, antlr4-python3-runtime\n",
      "  Building wheel for pylatexenc (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136897 sha256=41e592f9e6c77b8496e68516cfb4f55a1546f164f6efce777fe1995b4ff72a24\n",
      "  Stored in directory: /Users/henryadamcollie/Library/Caches/pip/wheels/3c/d9/c1/bb2a15d13c742b9035ef7ae6ebe236af270b1d1d9b386dcd5e\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=ac830519f99259c6330d16d9d044aacf6374418d96279064156f05256916464f\n",
      "  Stored in directory: /Users/henryadamcollie/Library/Caches/pip/wheels/d5/b3/74/a35b66048c9de6631cd74cbc9475e6feb3e69a467983446bd8\n",
      "Successfully built pylatexenc antlr4-python3-runtime\n",
      "Installing collected packages: pylatexenc, filetype, antlr4-python3-runtime, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-c, tree-sitter, soupsieve, shellingham, Shapely, scipy, rtree, rpds-py, pypdfium2, pyobjc-core, pyclipper, pluggy, pillow, opencv_python, omegaconf, mpire, mdurl, marko, lxml, latex2mathml, jsonref, faker, et-xmlfile, dill, defusedxml, colorlog, attrs, annotated-doc, torch, referencing, rapidocr, python-pptx, python-docx, pyobjc-framework-Cocoa, polyfactory, openpyxl, multiprocess, markdown-it-py, jsonlines, beautifulsoup4, torchvision, rich, pyobjc-framework-Quartz, pyobjc-framework-CoreML, pydantic-settings, jsonschema-specifications, accelerate, typer, semchunk, pyobjc-framework-Vision, jsonschema, ocrmac, docling-core, docling-parse, docling-ibm-models, docling\n",
      "\u001b[2K  Attempting uninstall: torchm\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/63\u001b[0m [et-xmlfile]on]\n",
      "\u001b[2K    Found existing installation: torch 2.9.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/63\u001b[0m [et-xmlfile]\n",
      "\u001b[2K    Uninstalling torch-2.9.1:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/63\u001b[0m [torch]]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.10m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/63\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63/63\u001b[0m [docling][docling][docling-parse]rk-Quartz]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Shapely-2.1.2 XlsxWriter-3.2.9 accelerate-1.12.0 annotated-doc-0.0.4 antlr4-python3-runtime-4.9.3 attrs-25.4.0 beautifulsoup4-4.14.3 colorlog-6.10.1 defusedxml-0.7.1 dill-0.4.1 docling-2.75.0 docling-core-2.66.0 docling-ibm-models-3.11.0 docling-parse-5.4.0 et-xmlfile-2.0.0 faker-40.5.1 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 latex2mathml-3.78.1 lxml-6.0.2 markdown-it-py-4.0.0 marko-2.2.2 mdurl-0.1.2 mpire-2.10.2 multiprocess-0.70.19 ocrmac-1.0.1 omegaconf-2.3.0 opencv_python-4.13.0.92 openpyxl-3.1.5 pillow-12.1.1 pluggy-1.6.0 polyfactory-3.3.0 pyclipper-1.4.0 pydantic-settings-2.13.1 pylatexenc-2.10 pyobjc-core-12.1 pyobjc-framework-Cocoa-12.1 pyobjc-framework-CoreML-12.1 pyobjc-framework-Quartz-12.1 pyobjc-framework-Vision-12.1 pypdfium2-5.5.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.6.0 referencing-0.37.0 rich-14.3.3 rpds-py-0.30.0 rtree-1.4.1 scipy-1.17.1 semchunk-2.2.2 shellingham-1.5.4 soupsieve-2.8.3 torch-2.10.0 torchvision-0.25.0 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.21.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf tqdm docling\n",
    "# Tesseract must be installed separately (system package, not pip):\n",
    "#   macOS:  brew install tesseract\n",
    "#   Linux:  sudo apt install tesseract-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract found: /opt/homebrew/bin/tesseract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Verify Tesseract is available\n",
    "if shutil.which(\"tesseract\"):\n",
    "    print(f\"Tesseract found: {shutil.which('tesseract')}\")\n",
    "else:\n",
    "    print(\"WARNING: Tesseract not found. OCR fallback will skip to Docling.\")\n",
    "    print(\"  Install: brew install tesseract  (macOS) or apt install tesseract-ocr (Linux)\")\n",
    "\n",
    "# Verify Docling is available\n",
    "try:\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    print(\"Docling found\")\n",
    "except ImportError:\n",
    "    print(\"WARNING: Docling not installed. Third fallback will be unavailable.\")\n",
    "    print(\"  Install: pip install docling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set the paths for your input PDFs and where you want the extracted text files written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION — update these paths to match your environment\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "PDF_INPUT_DIR = Path(\"/Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/pdfs/pdfs_pdfs\")          # directory containing your PDF files\n",
    "TEXT_OUTPUT_DIR = Path(\"/Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/pdfs/pdfs_text\")    # directory where .txt files will be written\n",
    "\n",
    "# Set to True to also extract per-page files (one .txt per page)\n",
    "SPLIT_BY_PAGE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Text Extractor (with OCR and Docling fallback)\n",
    "\n",
    "Each page is checked for embedded text first. If a page has no text but contains images, it falls back to Tesseract OCR. If Tesseract also fails (or isn't installed), Docling is used as a last resort on the entire document.\n",
    "\n",
    "**Extraction strategy per page:**\n",
    "1. `page.get_text()` — instant, works for native text PDFs\n",
    "2. `page.get_textpage_ocr()` — PyMuPDF's built-in Tesseract integration (fast OCR)\n",
    "3. Render page to PNG → `tesseract` subprocess — handles CCITTFax/JBIG2 1-bit fax images\n",
    "4. **Docling** — processes the full document when per-page methods fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_text(page):\n",
    "    \"\"\"Extract text from a single page, falling back to OCR if needed.\n",
    "\n",
    "    Three extraction paths are attempted:\n",
    "    1. get_text() — fast, works for native text PDFs.\n",
    "    2. get_textpage_ocr() — PyMuPDF built-in Tesseract OCR, works for most scanned pages.\n",
    "    3. pixmap → PNG → tesseract subprocess — slower but handles CCITTFax/JBIG2\n",
    "       1-bit fax images where get_textpage_ocr() silently returns empty.\n",
    "\n",
    "    Returns (text, method) where method is \"text\", \"ocr\", or \"empty\".\n",
    "    Pages returning \"empty\" are candidates for the Docling fallback.\n",
    "    \"\"\"\n",
    "    # Path 1: Native text extraction (instant)\n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        return text, \"text\"\n",
    "\n",
    "    # Path 2: PyMuPDF built-in OCR (fast, works for most image types)\n",
    "    try:\n",
    "        tp = page.get_textpage_ocr(tessdata=None, language=\"eng\", dpi=300)\n",
    "        ocr_text = page.get_text(textpage=tp)\n",
    "        if ocr_text.strip():\n",
    "            return ocr_text, \"ocr\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Path 3: Render page → PNG → Tesseract subprocess\n",
    "    # Needed for CCITTFax/DeviceGray 1-bit images (common in USPTO 2023+ grants)\n",
    "    # where get_textpage_ocr() fails silently.\n",
    "    try:\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f:\n",
    "            tmpname = f.name\n",
    "        pix.save(tmpname)\n",
    "        result = subprocess.run(\n",
    "            [\"tesseract\", tmpname, \"stdout\", \"--psm\", \"3\", \"-l\", \"eng\"],\n",
    "            capture_output=True, text=True, timeout=60,\n",
    "        )\n",
    "        os.unlink(tmpname)\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            return result.stdout, \"ocr\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\", \"empty\"\n",
    "\n",
    "\n",
    "def extract_with_docling(pdf_path: Path) -> str | None:\n",
    "    \"\"\"Last-resort extraction using Docling for the entire document.\n",
    "\n",
    "    Docling handles complex layouts, tables, and formats that\n",
    "    PyMuPDF and Tesseract may struggle with.\n",
    "\n",
    "    Returns the full document text, or None if Docling fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from docling.document_converter import DocumentConverter\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert(str(pdf_path))\n",
    "        text = result.document.export_to_markdown()\n",
    "        if text.strip():\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"  Docling failed for {pdf_path.name}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path, max_pages: int | None = None) -> tuple[str, dict]:\n",
    "    \"\"\"Extract text from a PDF using a 3-tier fallback strategy.\n",
    "\n",
    "    Tier 1: PyMuPDF get_text() per page (fast, native text)\n",
    "    Tier 2: Tesseract OCR per page (scanned/image pages)\n",
    "    Tier 3: Docling on the full document (if pages remain empty after tiers 1+2)\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file.\n",
    "        max_pages: If set, only extract the first N pages. None = all pages.\n",
    "\n",
    "    Returns (full_text, stats) where stats has page counts by method.\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages = []\n",
    "    stats = {\"text\": 0, \"ocr\": 0, \"docling\": 0, \"empty\": 0, \"total_in_pdf\": len(doc)}\n",
    "\n",
    "    page_limit = max_pages if max_pages is not None else len(doc)\n",
    "    empty_indices = []\n",
    "\n",
    "    # Tier 1 + 2: Try PyMuPDF text extraction and Tesseract OCR per page\n",
    "    for i, page in enumerate(doc):\n",
    "        if i >= page_limit:\n",
    "            break\n",
    "        page_text, method = extract_page_text(page)\n",
    "        pages.append(page_text)\n",
    "        if method == \"empty\":\n",
    "            empty_indices.append(i)\n",
    "        stats[method] += 1\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    # Tier 3: If any pages are still empty, try Docling on the full document\n",
    "    if empty_indices:\n",
    "        docling_text = extract_with_docling(pdf_path)\n",
    "        if docling_text:\n",
    "            # Docling returns the full document as one block — use it to fill empty pages.\n",
    "            # Since Docling doesn't give per-page output, we replace the first empty slot\n",
    "            # with the full Docling text and clear the rest to avoid duplication.\n",
    "            pages[empty_indices[0]] = docling_text\n",
    "            stats[\"docling\"] += 1\n",
    "            stats[\"empty\"] -= 1\n",
    "            for idx in empty_indices[1:]:\n",
    "                pages[idx] = \"\"\n",
    "                stats[\"docling\"] += 1\n",
    "                stats[\"empty\"] -= 1\n",
    "\n",
    "    return \"\\n\\n\".join(pages), stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-File Test\n",
    "\n",
    "Try extracting text from one PDF to verify everything works before processing a full directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first PDF in the input directory for a quick test\n",
    "pdf_files = sorted(PDF_INPUT_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF file(s) in {PDF_INPUT_DIR.resolve()}\")\n",
    "\n",
    "if pdf_files:\n",
    "    sample_pdf = pdf_files[0]\n",
    "    print(f\"\\nTesting with: {sample_pdf.name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    sample_text, stats = extract_text_from_pdf(sample_pdf)\n",
    "    extracted = stats[\"text\"] + stats[\"ocr\"] + stats[\"docling\"] + stats[\"empty\"]\n",
    "    print(f\"Extracted {extracted}/{stats['total_in_pdf']} pages\")\n",
    "    print(f\"  Text (PyMuPDF):  {stats['text']} pages\")\n",
    "    print(f\"  OCR (Tesseract): {stats['ocr']} pages\")\n",
    "    print(f\"  Docling:         {stats['docling']} pages\")\n",
    "    print(f\"  Empty:           {stats['empty']} pages\")\n",
    "    print(f\"Total characters: {len(sample_text):,}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Show a preview (first 2000 chars)\n",
    "    preview = sample_text[:2000]\n",
    "    print(preview)\n",
    "    if len(sample_text) > 2000:\n",
    "        print(f\"\\n... [{len(sample_text) - 2000:,} more characters]\")\n",
    "else:\n",
    "    print(f\"\\nNo PDF files found. Place your PDFs in: {PDF_INPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing\n",
    "\n",
    "Process all PDFs in the input directory and write `.txt` files to the output directory. Each output file keeps the same name as its source PDF but with a `.txt` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_files = sorted(PDF_INPUT_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Processing {len(pdf_files)} PDF(s) → {TEXT_OUTPUT_DIR.resolve()}\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Extracting text\"):\n",
    "    try:\n",
    "        if SPLIT_BY_PAGE:\n",
    "            # Per-page mode: extract page by page (Docling fallback still applies)\n",
    "            doc = pymupdf.open(pdf_path)\n",
    "            page_count = len(doc)\n",
    "            doc.close()\n",
    "            full_text, stats = extract_text_from_pdf(pdf_path)\n",
    "            # Write per-page files from the combined output\n",
    "            page_texts = full_text.split(\"\\n\\n\")\n",
    "            for i, page_text in enumerate(page_texts, start=1):\n",
    "                out_path = TEXT_OUTPUT_DIR / f\"{pdf_path.stem}_p{i}.txt\"\n",
    "                out_path.write_text(page_text, encoding=\"utf-8\")\n",
    "            char_count = len(full_text)\n",
    "        else:\n",
    "            full_text, stats = extract_text_from_pdf(pdf_path)\n",
    "            char_count = len(full_text)\n",
    "\n",
    "        # Always write the combined file\n",
    "        out_path = TEXT_OUTPUT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "        out_path.write_text(full_text, encoding=\"utf-8\")\n",
    "\n",
    "        # Determine primary method used\n",
    "        if stats[\"docling\"] > 0:\n",
    "            method = \"docling\"\n",
    "        elif stats[\"ocr\"] > 0:\n",
    "            method = \"ocr\"\n",
    "        else:\n",
    "            method = \"text\"\n",
    "\n",
    "        extracted = stats[\"text\"] + stats[\"ocr\"] + stats[\"docling\"] + stats[\"empty\"]\n",
    "\n",
    "        results.append({\n",
    "            \"file\": pdf_path.name,\n",
    "            \"pages_extracted\": extracted,\n",
    "            \"pages_total\": stats[\"total_in_pdf\"],\n",
    "            \"text_pages\": stats[\"text\"],\n",
    "            \"ocr_pages\": stats[\"ocr\"],\n",
    "            \"docling_pages\": stats[\"docling\"],\n",
    "            \"empty_pages\": stats[\"empty\"],\n",
    "            \"characters\": char_count,\n",
    "            \"method\": method,\n",
    "            \"status\": \"ok\" if char_count > 0 else \"empty\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"file\": pdf_path.name,\n",
    "            \"pages_extracted\": 0,\n",
    "            \"pages_total\": 0,\n",
    "            \"text_pages\": 0,\n",
    "            \"ocr_pages\": 0,\n",
    "            \"docling_pages\": 0,\n",
    "            \"empty_pages\": 0,\n",
    "            \"characters\": 0,\n",
    "            \"method\": \"error\",\n",
    "            \"status\": f\"error: {e}\"\n",
    "        })\n",
    "\n",
    "print(f\"\\nDone. {len(results)} file(s) processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"Total files:      {len(df)}\")\n",
    "    print(f\"Successful:       {(df['status'] == 'ok').sum()}\")\n",
    "    print(f\"Empty:            {(df['status'] == 'empty').sum()}\")\n",
    "    print(f\"Errors:           {df['status'].str.startswith('error').sum()}\")\n",
    "    print(f\"\\nExtraction method breakdown:\")\n",
    "    print(f\"  Text (PyMuPDF):  {(df['method'] == 'text').sum()} files\")\n",
    "    print(f\"  OCR (Tesseract): {(df['method'] == 'ocr').sum()} files\")\n",
    "    print(f\"  Docling:         {(df['method'] == 'docling').sum()} files\")\n",
    "    print(f\"\\nTotal pages:      {df['pages_extracted'].sum():,}\")\n",
    "    print(f\"  Text pages:      {df['text_pages'].sum():,}\")\n",
    "    print(f\"  OCR pages:       {df['ocr_pages'].sum():,}\")\n",
    "    print(f\"  Docling pages:   {df['docling_pages'].sum():,}\")\n",
    "    print(f\"  Empty pages:     {df['empty_pages'].sum():,}\")\n",
    "    print(f\"Total characters: {df['characters'].sum():,}\")\n",
    "    print()\n",
    "\n",
    "    empty = df[df[\"status\"] == \"empty\"].sort_values(\"file\")\n",
    "    if not empty.empty:\n",
    "        print(f\"\\n{len(empty)} empty files (no text extracted by any method):\")\n",
    "        for _, row in empty.iterrows():\n",
    "            print(f\"  {row['file']}  (total pages in PDF: {row['pages_total']})\")\n",
    "\n",
    "    errors = df[df[\"status\"].str.startswith(\"error\")]\n",
    "    if not errors.empty:\n",
    "        print(\"\\nFiles with errors:\")\n",
    "        for _, row in errors.iterrows():\n",
    "            print(f\"  {row['file']}: {row['status']}\")\n",
    "\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results — did you place PDF files in the input directory?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "The extracted `.txt` files are now ready for downstream processing:\n",
    "\n",
    "1. **Feed into Notebook 1** — if the PDFs contain email-like content, adapt the parser in Notebook 1 to read from the text output directory instead of `maildir/`\n",
    "2. **Direct Neo4j import** — load the text content as document nodes for full-text search and entity extraction\n",
    "3. **Entity extraction** — use spaCy NER or an LLM to pull out people, organizations, and other entities from the plain text\n",
    "\n",
    "### Extraction Pipeline\n",
    "\n",
    "```\n",
    "PDF file\n",
    "  │\n",
    "  ├─ Tier 1: PyMuPDF get_text()          [fast, native text]\n",
    "  │    └─ Got text? ✓ Done\n",
    "  │\n",
    "  ├─ Tier 2: Tesseract OCR               [scanned/image pages]\n",
    "  │    ├─ get_textpage_ocr() built-in\n",
    "  │    └─ pixmap → PNG → tesseract CLI\n",
    "  │         └─ Got text? ✓ Done\n",
    "  │\n",
    "  └─ Tier 3: Docling                     [complex layouts, last resort]\n",
    "       └─ Full document conversion\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Empty text output (all tiers fail) | Check Tesseract: `tesseract --version`. Check Docling: `pip install docling` |\n",
    "| Tesseract not found | `brew install tesseract` (macOS) or `apt install tesseract-ocr` (Linux) |\n",
    "| Docling import error | `pip install docling` — requires Python 3.9+ |\n",
    "| Slow processing | OCR is ~2-5s/page, Docling ~5-10s/page. Text-based PDFs are instant. |\n",
    "| Garbled OCR text | Try increasing DPI in `extract_page_text` (e.g. `dpi=400`) |\n",
    "| Non-English PDFs | Install Tesseract language packs: `brew install tesseract-lang`, change `language=\"eng\"` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
