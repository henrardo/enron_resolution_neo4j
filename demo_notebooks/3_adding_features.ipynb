{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae4d3a0",
   "metadata": {},
   "source": [
    "# Improving entity resolution\n",
    "\n",
    "In the previous notebook, we attempted to perform entity resolution with little data.\n",
    "\n",
    "While Louvain + Jaro-Winkler did produce near-perfect results on a relatively large set of entities, there are things we can do to improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023235b8",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "In an ideal world, your data would include stable unique identifiers like:\n",
    "\n",
    "- Social Security Numbers\n",
    "- Phone numbers\n",
    "- Addresses\n",
    "\n",
    "And definable attributes, like:\n",
    "- Age\n",
    "- Gender\n",
    "- Job role\n",
    "\n",
    "The more of these identifiers and attributes we have, the easier it is to resolve an entity.\n",
    "\n",
    "Let's say we had a PhoneNumber node and an SSN node. In our dataset, some Person nodes are connected to both, and some only to one. To resolve these, we can simply run WCC on a projection of these nodes and rels, do some light name-matching and push them to a Parent node.\n",
    "\n",
    "However, real-life is rarely so kind -- especially when dealing with unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696f009",
   "metadata": {},
   "source": [
    "## Inferring features\n",
    "\n",
    "In the previous notebook, we did not consider only one aspect of each User. Instead, we considered several features of each user.\n",
    "\n",
    "Think of each feature as a voter. The more voters we have, with different perspectives, the more accurate our resolutions.\n",
    "\n",
    "In this notebook, we'll try to infer some new feature nodes to use as voting blocks for resolution, inclulding:\n",
    "\n",
    "- Stylometry\n",
    "- Etc.\n",
    "- Etc.\n",
    "\n",
    "First, as always, we'll connect to the database. \n",
    "\n",
    "Then, we'll clean up the resolution artifacts from the previous notebook -- we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491503a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20a0a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4d5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j GDS 2.23.0\n",
      "Found 517,401 emails in database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "import os\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "DATABASE = \"enrondemo\"\n",
    "\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"NEO4J_PASSWORD not found in .env file!\")\n",
    "\n",
    "try:\n",
    "    gds = GraphDataScience(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD), database=DATABASE)\n",
    "    gds_version = gds.version()\n",
    "    print(f\"Connected to Neo4j GDS {gds_version}\")\n",
    "    \n",
    "    # Verify data exists\n",
    "    result = gds.run_cypher(\"MATCH (e:Email) RETURN count(e) as count\")\n",
    "    email_count = result['count'].iloc[0]\n",
    "    \n",
    "    if email_count == 0:\n",
    "        raise ValueError(\"No emails found! Please run Notebook 1 first to import data.\")\n",
    "    \n",
    "    print(f\"Found {email_count:,} emails in database\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection or data validation failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Have you run Notebook 1 to import data?\")\n",
    "    print(\"  2. Is Neo4j running with the 'neo4j database?\")\n",
    "    print(\"  3. Is the GDS plugin installed and activated?\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84386bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up the resolution artifacts from the previous notebook\n",
    "delete_email_properties = \"\"\"\n",
    "MATCH (e:Email)\n",
    "CALL (e) {\n",
    "    REMOVE e.dateFloat,\n",
    "           e.degree_centrality_undirected,\n",
    "           e.fastrp_embedding_full,\n",
    "           e.fastrp_embedding_scaled,\n",
    "           e.louvain_community,\n",
    "           e.scaledFeatures,\n",
    "           e.fastrp_embedding_features,\n",
    "           e.wcc_id\n",
    "} IN TRANSACTIONS OF 10000 ROWS\n",
    "\"\"\"\n",
    "\n",
    "delete_user_properties = \"\"\"\n",
    "MATCH (u:User)\n",
    "CALL (u) {\n",
    "    REMOVE u.degree_centrality_undirected,\n",
    "           u.email_degree,\n",
    "           u.fastrp_embedding_full,\n",
    "           u.fastrp_embedding_scaled,\n",
    "           u.fastrp_embedding_features,\n",
    "           u.louvain_community,\n",
    "           u.scaledFeatures,\n",
    "           u.wcc_id\n",
    "} IN TRANSACTIONS OF 10000 ROWS\n",
    "\"\"\"\n",
    "\n",
    "delete_mailbox_properties = \"\"\"\n",
    "MATCH (m:Mailbox)\n",
    "CALL (m) {\n",
    "    REMOVE m.betweenness,\n",
    "           m.leiden_community,\n",
    "           m.pagerank,\n",
    "           m.degree_centrality_undirected,\n",
    "           m.email_degree,\n",
    "           m.fastrp_embedding_full,\n",
    "           m.fastrp_embedding_scaled,\n",
    "           m.fastrp_embedding_features,\n",
    "           m.louvain_community,\n",
    "           m.scaledFeatures,\n",
    "           m.wcc_id\n",
    "} IN TRANSACTIONS OF 10000 ROWS\n",
    "\"\"\"\n",
    "\n",
    "gds.run_cypher(delete_email_properties)\n",
    "gds.run_cypher(delete_user_properties)\n",
    "gds.run_cypher(delete_mailbox_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc547f23",
   "metadata": {},
   "source": [
    "# 1. Stylometry\n",
    "\n",
    "Although a relatively weak signal at the smaller scale, the attributes of a person's writing style can become useful artifacts in and of themselves.\n",
    "\n",
    "From email content alone we can find the following fingerprints:\n",
    "- Lexical\n",
    "- Syntactic\n",
    "- Character-level (n-grams, capitalisation, whitespace)\n",
    "- Structural (paragraph length, greeting/signoff patterns, line breaks)\n",
    "- Function words\n",
    "\n",
    "Take the following email for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56119e22",
   "metadata": {},
   "source": [
    "First, we'll create an index on our name properties. This will improve the speed of our text distance checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a4c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "index_query = \"\"\"\n",
    "// Create fulltext index on User name properties\n",
    "CREATE FULLTEXT INDEX user_names_fulltext IF NOT EXISTS\n",
    "FOR (u:User)\n",
    "ON EACH [u.nameNormStrip, u.nameRaw]\n",
    "\"\"\"\n",
    "\n",
    "index_results = gds.run_cypher(index_query)\n",
    "print(index_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c001cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading STAR model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load STAR model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load STAR model (if not already loaded)\n",
    "print(\"Loading STAR model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('AIDA-UPM/star')\n",
    "model = AutoModel.from_pretrained('AIDA-UPM/star')\n",
    "model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32243b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAR define functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_style_embedding(text: str, max_length: int = 512) -> list:\n",
    "    \"\"\"Generate a 768-dimensional stylometric embedding using STAR.\"\"\"\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return None\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        max_length=max_length,\n",
    "        padding=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.pooler_output.squeeze().tolist()\n",
    "\n",
    "def cosine_similarity(v1: list, v2: list) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    if v1 is None or v2 is None:\n",
    "        return None\n",
    "    import math\n",
    "    dot = sum(a * b for a, b in zip(v1, v2))\n",
    "    norm1 = math.sqrt(sum(a * a for a in v1))\n",
    "    norm2 = math.sqrt(sum(b * b for b in v2))\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def extract_first_segment(thread_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract only the first segment of an email thread.\n",
    "    Removes forwarded/replied content to get clean authored text.\n",
    "    \"\"\"\n",
    "    if not thread_text:\n",
    "        return \"\"\n",
    "    \n",
    "    markers = [\n",
    "        '---------------------- Forwarded by',\n",
    "        '----- Forwarded by',\n",
    "        '-----Original Message-----',\n",
    "        '---------------------- Original',\n",
    "        'From:',  # Often marks the start of forwarded headers\n",
    "    ]\n",
    "    \n",
    "    text = thread_text\n",
    "    for marker in markers:\n",
    "        if marker in text:\n",
    "            parts = text.split(marker)\n",
    "            # Take everything before the first marker\n",
    "            text = parts[0]\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a55aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Neo4j using fulltext fuzzy search...\n",
      "Found 13 mailbox records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_normalized</th>\n",
       "      <th>mailbox</th>\n",
       "      <th>email_count</th>\n",
       "      <th>match_score</th>\n",
       "      <th>threads</th>\n",
       "      <th>community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>12.301496</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>566080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth \" Lay</td>\n",
       "      <td>Kenneth \" Lay</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>12.301496</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>132149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth Lay (E-mail)</td>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>10.685865</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>132149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth L. Lay</td>\n",
       "      <td>Kenneth L. Lay</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>10.423277</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>495222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth L. Lay (E-mail)</td>\n",
       "      <td>Kenneth L. Lay</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>9.200720</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>132149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kenneth Lay</td>\n",
       "      <td>Kenneth L. Lay - Enron</td>\n",
       "      <td>Kenneth L. Lay - Enron</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>9.043266</td>\n",
       "      <td>[Message-ID: &lt;4205652.1075859390811.JavaMail.e...</td>\n",
       "      <td>132149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>rick.buy@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>13.503496</td>\n",
       "      <td>[Message-ID: &lt;10973933.1075859677452.JavaMail....</td>\n",
       "      <td>566080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>buy@enron.com</td>\n",
       "      <td>1</td>\n",
       "      <td>13.503496</td>\n",
       "      <td>[Message-ID: &lt;316659.1075857666899.JavaMail.ev...</td>\n",
       "      <td>566080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy- Enron Corp. Chief Risk Officer</td>\n",
       "      <td>Rick Buy- Enron Corp. Chief Risk Officer</td>\n",
       "      <td>no.address@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>9.978207</td>\n",
       "      <td>[Message-ID: &lt;15394323.1075840430245.JavaMail....</td>\n",
       "      <td>487657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy- Enron Corp. Chief Risk Officer</td>\n",
       "      <td>Rick Buy- Enron Corp. Chief Risk Officer</td>\n",
       "      <td>40enron@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>9.978207</td>\n",
       "      <td>[Message-ID: &lt;7671344.1075840430472.JavaMail.e...</td>\n",
       "      <td>487657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy and Mark Haedicke</td>\n",
       "      <td>Rick Buy And Mark Haedicke</td>\n",
       "      <td>40enron@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>8.766623</td>\n",
       "      <td>[Message-ID: &lt;7671344.1075840430472.JavaMail.e...</td>\n",
       "      <td>487657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy and Mark Haedicke</td>\n",
       "      <td>Rick Buy And Mark Haedicke</td>\n",
       "      <td>no.address@enron.com</td>\n",
       "      <td>30</td>\n",
       "      <td>8.766623</td>\n",
       "      <td>[Message-ID: &lt;15394323.1075840430245.JavaMail....</td>\n",
       "      <td>487657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rick Buy</td>\n",
       "      <td>Rick Buy and Mark Haedicke</td>\n",
       "      <td>Rick Buy And Mark Haedicke</td>\n",
       "      <td>rick.haedicke@enron.com</td>\n",
       "      <td>1</td>\n",
       "      <td>8.766623</td>\n",
       "      <td>[Message-ID: &lt;8809910.1075852107703.JavaMail.e...</td>\n",
       "      <td>591513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    search_term                                 user_name  \\\n",
       "0   Kenneth Lay                               Kenneth Lay   \n",
       "1   Kenneth Lay                             Kenneth \" Lay   \n",
       "2   Kenneth Lay                      Kenneth Lay (E-mail)   \n",
       "3   Kenneth Lay                            Kenneth L. Lay   \n",
       "4   Kenneth Lay                   Kenneth L. Lay (E-mail)   \n",
       "5   Kenneth Lay                    Kenneth L. Lay - Enron   \n",
       "6      Rick Buy                                  Rick Buy   \n",
       "7      Rick Buy                                  Rick Buy   \n",
       "8      Rick Buy  Rick Buy- Enron Corp. Chief Risk Officer   \n",
       "9      Rick Buy  Rick Buy- Enron Corp. Chief Risk Officer   \n",
       "10     Rick Buy                Rick Buy and Mark Haedicke   \n",
       "11     Rick Buy                Rick Buy and Mark Haedicke   \n",
       "12     Rick Buy                Rick Buy and Mark Haedicke   \n",
       "\n",
       "                             user_normalized                  mailbox  \\\n",
       "0                                Kenneth Lay    kenneth.lay@enron.com   \n",
       "1                              Kenneth \" Lay    kenneth.lay@enron.com   \n",
       "2                                Kenneth Lay    kenneth.lay@enron.com   \n",
       "3                             Kenneth L. Lay    kenneth.lay@enron.com   \n",
       "4                             Kenneth L. Lay    kenneth.lay@enron.com   \n",
       "5                     Kenneth L. Lay - Enron    kenneth.lay@enron.com   \n",
       "6                                   Rick Buy       rick.buy@enron.com   \n",
       "7                                   Rick Buy            buy@enron.com   \n",
       "8   Rick Buy- Enron Corp. Chief Risk Officer     no.address@enron.com   \n",
       "9   Rick Buy- Enron Corp. Chief Risk Officer        40enron@enron.com   \n",
       "10                Rick Buy And Mark Haedicke        40enron@enron.com   \n",
       "11                Rick Buy And Mark Haedicke     no.address@enron.com   \n",
       "12                Rick Buy And Mark Haedicke  rick.haedicke@enron.com   \n",
       "\n",
       "    email_count  match_score  \\\n",
       "0            30    12.301496   \n",
       "1            30    12.301496   \n",
       "2            30    10.685865   \n",
       "3            30    10.423277   \n",
       "4            30     9.200720   \n",
       "5            30     9.043266   \n",
       "6            30    13.503496   \n",
       "7             1    13.503496   \n",
       "8            30     9.978207   \n",
       "9            30     9.978207   \n",
       "10           30     8.766623   \n",
       "11           30     8.766623   \n",
       "12            1     8.766623   \n",
       "\n",
       "                                              threads  community  \n",
       "0   [Message-ID: <4205652.1075859390811.JavaMail.e...     566080  \n",
       "1   [Message-ID: <4205652.1075859390811.JavaMail.e...     132149  \n",
       "2   [Message-ID: <4205652.1075859390811.JavaMail.e...     132149  \n",
       "3   [Message-ID: <4205652.1075859390811.JavaMail.e...     495222  \n",
       "4   [Message-ID: <4205652.1075859390811.JavaMail.e...     132149  \n",
       "5   [Message-ID: <4205652.1075859390811.JavaMail.e...     132149  \n",
       "6   [Message-ID: <10973933.1075859677452.JavaMail....     566080  \n",
       "7   [Message-ID: <316659.1075857666899.JavaMail.ev...     566080  \n",
       "8   [Message-ID: <15394323.1075840430245.JavaMail....     487657  \n",
       "9   [Message-ID: <7671344.1075840430472.JavaMail.e...     487657  \n",
       "10  [Message-ID: <7671344.1075840430472.JavaMail.e...     487657  \n",
       "11  [Message-ID: <15394323.1075840430245.JavaMail....     487657  \n",
       "12  [Message-ID: <8809910.1075852107703.JavaMail.e...     591513  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STAR Fingerprinting: Compare Rick Buy vs Kenneth Lay mailboxes\n",
    "# =============================================================================\n",
    "\n",
    "query = \"\"\"\n",
    "// Target names to search\n",
    "WITH ['Rick Buy', 'Kenneth Lay'] AS target_names\n",
    "UNWIND target_names AS target_name\n",
    "\n",
    "// Split name into words and add fuzzy threshold\n",
    "WITH target_name,\n",
    "     [word IN split(target_name, ' ') | word + '~0.7'] AS fuzzy_words\n",
    "\n",
    "// Query fulltext index with fuzzy matching\n",
    "CALL db.index.fulltext.queryNodes('user_names_fulltext', apoc.text.join(fuzzy_words, ' AND '))\n",
    "YIELD node AS u, score\n",
    "\n",
    "// Filter by score threshold\n",
    "WHERE score > 1.0\n",
    "\n",
    "// Get their mailboxes\n",
    "MATCH (u)-[:USED]->(m:Mailbox)\n",
    "\n",
    "// Get emails SENT from this mailbox\n",
    "MATCH (m)-[:SENT]->(e:Email)\n",
    "\n",
    "// Aggregate\n",
    "WITH target_name, u, m, score, collect(e.thread)[0..30] AS threads\n",
    "\n",
    "RETURN \n",
    "    target_name AS search_term,\n",
    "    u.nameRaw AS user_name,\n",
    "    u.nameNormStrip AS user_normalized,\n",
    "    m.address AS mailbox,\n",
    "    size(threads) AS email_count,\n",
    "    score AS match_score,\n",
    "    threads,\n",
    "    u.louvain_community AS community\n",
    "ORDER BY target_name, score DESC, size(threads) DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying Neo4j using fulltext fuzzy search...\")\n",
    "results = gds.run_cypher(query)\n",
    "print(f\"Found {len(results)} mailbox records\")\n",
    "results.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating STAR embeddings with FIXED extraction...\n",
      "----------------------------------------------------------------------\n",
      "  Kenneth Lay               | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Kenneth \" Lay             | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Kenneth Lay               | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Kenneth L. Lay            | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Kenneth L. Lay            | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Kenneth L. Lay - Enron    | kenneth.lay@enron.com               |  30 emails |  8719 chars ✓\n",
      "  Rick Buy                  | rick.buy@enron.com                  |  30 emails |  9635 chars ✓\n",
      "  Rick Buy                  | buy@enron.com                       |   1 emails |   315 chars ✓\n",
      "  Rick Buy- Enron Corp. Chi | no.address@enron.com                |  30 emails | 15836 chars ✓\n",
      "  Rick Buy- Enron Corp. Chi | 40enron@enron.com                   |  30 emails | 16105 chars ✓\n",
      "  Rick Buy And Mark Haedick | 40enron@enron.com                   |  30 emails | 16105 chars ✓\n",
      "  Rick Buy And Mark Haedick | no.address@enron.com                |  30 emails | 15836 chars ✓\n",
      "  Rick Buy And Mark Haedick | rick.haedicke@enron.com             |   1 emails |  1111 chars ✓\n",
      "----------------------------------------------------------------------\n",
      "Generated 13 embeddings from 13 User-Mailbox pairs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Generate STAR embeddings with fixed extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_email_body(thread_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the actual email body content, stripping headers and forwarded content.\n",
    "    \"\"\"\n",
    "    if not thread_text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = thread_text\n",
    "    \n",
    "    # Parse past headers to find body\n",
    "    lines = text.split('\\n')\n",
    "    in_headers = True\n",
    "    body_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if in_headers:\n",
    "            # Header lines: \"Field: Value\" or whitespace continuation\n",
    "            if ':' in line and line.split(':')[0].replace('-', '').replace('_', '').replace(' ', '').isalnum():\n",
    "                continue\n",
    "            elif line.startswith(' ') or line.startswith('\\t'):\n",
    "                continue\n",
    "            elif line.strip() == '':\n",
    "                in_headers = False\n",
    "                continue\n",
    "            else:\n",
    "                in_headers = False\n",
    "                body_lines.append(line)\n",
    "        else:\n",
    "            body_lines.append(line)\n",
    "    \n",
    "    body_text = '\\n'.join(body_lines)\n",
    "    \n",
    "    # Remove forwarded/replied content\n",
    "    forward_markers = [\n",
    "        '---------------------- Forwarded by',\n",
    "        '----- Forwarded by',\n",
    "        '-----Original Message-----',\n",
    "        '---------------------- Original',\n",
    "        '----- Original Message -----',\n",
    "        '________________________________________',\n",
    "    ]\n",
    "    \n",
    "    for marker in forward_markers:\n",
    "        if marker in body_text:\n",
    "            body_text = body_text.split(marker)[0]\n",
    "    \n",
    "    return body_text.strip()\n",
    "\n",
    "\n",
    "def concatenate_emails(threads: list, max_chars: int = 15000) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate email bodies for fingerprinting.\n",
    "    \"\"\"\n",
    "    if not threads:\n",
    "        return \"\"\n",
    "    \n",
    "    segments = []\n",
    "    total_chars = 0\n",
    "    \n",
    "    for thread in threads:\n",
    "        if thread is None:\n",
    "            continue\n",
    "        segment = extract_email_body(thread)\n",
    "        if len(segment) > 30:  # Lowered threshold - short emails still have style\n",
    "            segments.append(segment)\n",
    "            total_chars += len(segment)\n",
    "            if total_chars > max_chars:\n",
    "                break\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(segments)\n",
    "\n",
    "\n",
    "# Process each row (no deduplication - each User-Mailbox pair is unique)\n",
    "fingerprints = []\n",
    "\n",
    "print(\"Generating STAR embeddings with FIXED extraction...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    combined_text = concatenate_emails(row['threads'])\n",
    "    embedding = get_style_embedding(combined_text) if combined_text and len(combined_text) > 100 else None\n",
    "    \n",
    "    fingerprints.append({\n",
    "        'search_term': row['search_term'],\n",
    "        'user_name': row['user_name'],\n",
    "        'user_normalized': row['user_normalized'],\n",
    "        'mailbox': row['mailbox'],\n",
    "        'email_count': row['email_count'],\n",
    "        'text_length': len(combined_text) if combined_text else 0,\n",
    "        'embedding': embedding,\n",
    "        'community': row['community']\n",
    "    })\n",
    "    \n",
    "    status = \"✓\" if embedding else \"✗ (insufficient text)\"\n",
    "    print(f\"  {row['user_normalized'][:25]:<25} | {row['mailbox']:<35} | {row['email_count']:>3} emails | {len(combined_text) if combined_text else 0:>5} chars {status}\")\n",
    "\n",
    "valid_embeddings = len([f for f in fingerprints if f['embedding']])\n",
    "print(\"-\" * 70)\n",
    "print(f\"Generated {valid_embeddings} embeddings from {len(fingerprints)} User-Mailbox pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5cac689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "STYLOMETRIC SIMILARITY TO Rick Buy (rick.buy@enron.com)\n",
      "===============================================================================================\n",
      "\n",
      "| Search Term   | User                                     | Mailbox                 |   Community |   Emails |   Text Chars |   Style Similarity |\n",
      "|:--------------|:-----------------------------------------|:------------------------|------------:|---------:|-------------:|-------------------:|\n",
      "| Rick Buy      | Rick Buy                                 | rick.buy@enron.com      |      566080 |       30 |         9635 |             1      |\n",
      "| Rick Buy      | Rick Buy                                 | buy@enron.com           |      566080 |        1 |          315 |             0.7392 |\n",
      "| Kenneth Lay   | Kenneth Lay                              | kenneth.lay@enron.com   |      566080 |       30 |         8719 |             0.6983 |\n",
      "| Kenneth Lay   | Kenneth \" Lay                            | kenneth.lay@enron.com   |      132149 |       30 |         8719 |             0.6983 |\n",
      "| Kenneth Lay   | Kenneth Lay (E-mail)                     | kenneth.lay@enron.com   |      132149 |       30 |         8719 |             0.6983 |\n",
      "| Kenneth Lay   | Kenneth L. Lay                           | kenneth.lay@enron.com   |      495222 |       30 |         8719 |             0.6983 |\n",
      "| Kenneth Lay   | Kenneth L. Lay (E-mail)                  | kenneth.lay@enron.com   |      132149 |       30 |         8719 |             0.6983 |\n",
      "| Kenneth Lay   | Kenneth L. Lay - Enron                   | kenneth.lay@enron.com   |      132149 |       30 |         8719 |             0.6983 |\n",
      "| Rick Buy      | Rick Buy- Enron Corp. Chief Risk Officer | 40enron@enron.com       |      487657 |       30 |        16105 |             0.6957 |\n",
      "| Rick Buy      | Rick Buy and Mark Haedicke               | 40enron@enron.com       |      487657 |       30 |        16105 |             0.6957 |\n",
      "| Rick Buy      | Rick Buy- Enron Corp. Chief Risk Officer | no.address@enron.com    |      487657 |       30 |        15836 |             0.6633 |\n",
      "| Rick Buy      | Rick Buy and Mark Haedicke               | no.address@enron.com    |      487657 |       30 |        15836 |             0.6633 |\n",
      "| Rick Buy      | Rick Buy and Mark Haedicke               | rick.haedicke@enron.com |      591513 |        1 |         1111 |             0.6392 |\n",
      "\n",
      "--- Summary (mailboxes with 500+ chars) ---\n",
      "Rick Buy avg similarity:    0.7262 (n=6)\n",
      "Kenneth Lay avg similarity: 0.6983 (n=6)\n",
      "Separation:                 +0.0279\n",
      "\n",
      "STAR successfully distinguishes Rick Buy from Kenneth Lay!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Compare all to rick.buy@enron.com (with copy-pasteable output)\n",
    "# =============================================================================\n",
    "\n",
    "# Find Rick Buy reference\n",
    "reference_mailbox = 'rick.buy@enron.com'\n",
    "reference_embedding = None\n",
    "reference_user = None\n",
    "\n",
    "for fp in fingerprints:\n",
    "    if fp['mailbox'] == reference_mailbox and fp['embedding'] is not None:\n",
    "        if fp['user_normalized'] == 'Rick Buy':\n",
    "            reference_embedding = fp['embedding']\n",
    "            reference_user = fp['user_name']\n",
    "            break\n",
    "\n",
    "if reference_embedding is None:\n",
    "    print(\"ERROR: No embedding for rick.buy@enron.com\")\n",
    "else:\n",
    "    comparison_results = []\n",
    "    \n",
    "    for fp in fingerprints:\n",
    "        similarity = cosine_similarity(reference_embedding, fp['embedding'])\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Search Term': fp['search_term'],\n",
    "            'User': fp['user_name'],\n",
    "            'Mailbox': fp['mailbox'],\n",
    "            'Community': fp['community'],\n",
    "            'Emails': fp['email_count'],\n",
    "            'Text Chars': fp['text_length'],\n",
    "            'Style Similarity': round(similarity, 4) if similarity else None\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_results)\n",
    "    df_comparison = df_comparison.sort_values('Style Similarity', ascending=False, na_position='last')\n",
    "    \n",
    "    # Print as markdown table\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"STYLOMETRIC SIMILARITY TO {reference_user} ({reference_mailbox})\")\n",
    "    print(\"=\" * 95)\n",
    "    print()\n",
    "    print(df_comparison.to_markdown(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Summary with sufficient text only\n",
    "    df_valid = df_comparison[df_comparison['Text Chars'] >= 500]\n",
    "    rick_sims = df_valid[df_valid['Search Term'] == 'Rick Buy']['Style Similarity'].dropna()\n",
    "    ken_sims = df_valid[df_valid['Search Term'] == 'Kenneth Lay']['Style Similarity'].dropna()\n",
    "    \n",
    "    print(f\"--- Summary (mailboxes with 500+ chars) ---\")\n",
    "    print(f\"Rick Buy avg similarity:    {rick_sims.mean():.4f} (n={len(rick_sims)})\")\n",
    "    print(f\"Kenneth Lay avg similarity: {ken_sims.mean():.4f} (n={len(ken_sims)})\")\n",
    "    print(f\"Separation:                 {rick_sims.mean() - ken_sims.mean():+.4f}\")\n",
    "    \n",
    "    if rick_sims.mean() > ken_sims.mean():\n",
    "        print(\"\\nSTAR successfully distinguishes Rick Buy from Kenneth Lay!\")\n",
    "    else:\n",
    "        print(\"\\nStyles not clearly separated - may need more data or different approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab52c0",
   "metadata": {},
   "source": [
    "## Stylometric workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddff8",
   "metadata": {},
   "source": [
    "First let's get a sense of how many emails we're going to embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c24c7a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing dataset scale...\n",
      "|   total_mailboxes |   total_emails |   avg_emails_per_mailbox |   median_emails |   p90_emails |   max_emails |\n",
      "|------------------:|---------------:|-------------------------:|----------------:|-------------:|-------------:|\n",
      "|             20311 |         517399 |                  25.4738 |               3 |           25 |        16735 |\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Assess scale - how many mailboxes have sent emails?\n",
    "# =============================================================================\n",
    "\n",
    "scale_query = \"\"\"\n",
    "MATCH (m:Mailbox)-[:SENT]->(e:Email)\n",
    "WITH m, count(e) AS email_count\n",
    "RETURN \n",
    "    count(m) AS total_mailboxes,\n",
    "    sum(email_count) AS total_emails,\n",
    "    avg(email_count) AS avg_emails_per_mailbox,\n",
    "    percentileCont(email_count, 0.5) AS median_emails,\n",
    "    percentileCont(email_count, 0.9) AS p90_emails,\n",
    "    max(email_count) AS max_emails\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assessing dataset scale...\")\n",
    "scale = gds.run_cypher(scale_query)\n",
    "print(scale.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb75b9",
   "metadata": {},
   "source": [
    "Next we need to acquire only the mailboxes that have sent enough emails to obtain adequate fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "221a335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching mailboxes with >= 3 sent emails...\n",
      "Found 10272 mailboxes to fingerprint\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Get all mailboxes with sufficient emails for fingerprinting\n",
    "# =============================================================================\n",
    "\n",
    "MIN_EMAILS = 3\n",
    "\n",
    "mailbox_query = f\"\"\"\n",
    "MATCH (m:Mailbox)-[:SENT]->(e:Email)\n",
    "WITH m, collect(e.thread) AS threads, count(e) AS email_count\n",
    "WHERE email_count >= {MIN_EMAILS}\n",
    "\n",
    "// Get the User(s) associated with this mailbox for reference\n",
    "OPTIONAL MATCH (u:User)-[:USED]->(m)\n",
    "\n",
    "RETURN \n",
    "    m.address AS mailbox,\n",
    "    email_count,\n",
    "    threads,\n",
    "    collect(DISTINCT u.nameNormStrip)[0..3] AS associated_users,\n",
    "    collect(DISTINCT u.louvain_community)[0..3] AS communities\n",
    "ORDER BY email_count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Fetching mailboxes with >= {MIN_EMAILS} sent emails...\")\n",
    "mailbox_data = gds.run_cypher(mailbox_query)\n",
    "print(f\"Found {len(mailbox_data)} mailboxes to fingerprint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6359f",
   "metadata": {},
   "source": [
    "Next we will use the STAR model to generate embeddings for our concatenated emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20af36f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating STAR embeddings for 10272 mailboxes (TOP MESSAGE ONLY)...\n",
      "----------------------------------------------------------------------\n",
      "  Processing 0/10272 (0.0/sec)...\n",
      "  Processing 100/10272 (3.5/sec)...\n",
      "  Processing 200/10272 (3.5/sec)...\n",
      "  Processing 300/10272 (3.5/sec)...\n",
      "  Processing 400/10272 (3.5/sec)...\n",
      "  Processing 500/10272 (3.5/sec)...\n",
      "  Processing 600/10272 (3.5/sec)...\n",
      "  Processing 700/10272 (3.5/sec)...\n",
      "  Processing 800/10272 (3.5/sec)...\n",
      "  Processing 900/10272 (3.5/sec)...\n",
      "  Processing 1000/10272 (3.4/sec)...\n",
      "  Processing 1100/10272 (3.4/sec)...\n",
      "  Processing 1200/10272 (3.5/sec)...\n",
      "  Processing 1300/10272 (3.5/sec)...\n",
      "  Processing 1400/10272 (3.5/sec)...\n",
      "  Processing 1500/10272 (3.5/sec)...\n",
      "  Processing 1600/10272 (3.5/sec)...\n",
      "  Processing 1700/10272 (3.5/sec)...\n",
      "  Processing 1800/10272 (3.5/sec)...\n",
      "  Processing 1900/10272 (3.5/sec)...\n",
      "  Processing 2000/10272 (3.5/sec)...\n",
      "  Processing 2100/10272 (3.5/sec)...\n",
      "  Processing 2200/10272 (3.5/sec)...\n",
      "  Processing 2300/10272 (3.5/sec)...\n",
      "  Processing 2400/10272 (3.5/sec)...\n",
      "  Processing 2500/10272 (3.5/sec)...\n",
      "  Processing 2600/10272 (3.5/sec)...\n",
      "  Processing 2700/10272 (3.5/sec)...\n",
      "  Processing 2800/10272 (3.5/sec)...\n",
      "  Processing 2900/10272 (3.5/sec)...\n",
      "  Processing 3000/10272 (3.5/sec)...\n",
      "  Processing 3100/10272 (3.5/sec)...\n",
      "  Processing 3200/10272 (3.5/sec)...\n",
      "  Processing 3300/10272 (3.5/sec)...\n",
      "  Processing 3400/10272 (3.5/sec)...\n",
      "  Processing 3500/10272 (3.6/sec)...\n",
      "  Processing 3600/10272 (3.6/sec)...\n",
      "  Processing 3700/10272 (3.6/sec)...\n",
      "  Processing 3800/10272 (3.6/sec)...\n",
      "  Processing 3900/10272 (3.6/sec)...\n",
      "  Processing 4000/10272 (3.6/sec)...\n",
      "  Processing 4100/10272 (3.6/sec)...\n",
      "  Processing 4200/10272 (3.6/sec)...\n",
      "  Processing 4300/10272 (3.6/sec)...\n",
      "  Processing 4400/10272 (3.6/sec)...\n",
      "  Processing 4500/10272 (3.6/sec)...\n",
      "  Processing 4600/10272 (3.6/sec)...\n",
      "  Processing 4700/10272 (3.6/sec)...\n",
      "  Processing 4800/10272 (3.6/sec)...\n",
      "  Processing 4900/10272 (3.6/sec)...\n",
      "  Processing 5000/10272 (3.6/sec)...\n",
      "  Processing 5100/10272 (3.7/sec)...\n",
      "  Processing 5200/10272 (3.7/sec)...\n",
      "  Processing 5300/10272 (3.7/sec)...\n",
      "  Processing 5400/10272 (3.7/sec)...\n",
      "  Processing 5500/10272 (3.7/sec)...\n",
      "  Processing 5600/10272 (3.7/sec)...\n",
      "  Processing 5700/10272 (3.7/sec)...\n",
      "  Processing 5800/10272 (3.8/sec)...\n",
      "  Processing 5900/10272 (3.8/sec)...\n",
      "  Processing 6000/10272 (3.8/sec)...\n",
      "  Processing 6100/10272 (3.8/sec)...\n",
      "  Processing 6200/10272 (3.8/sec)...\n",
      "  Processing 6300/10272 (3.8/sec)...\n",
      "  Processing 6400/10272 (3.8/sec)...\n",
      "  Processing 6500/10272 (3.9/sec)...\n",
      "  Processing 6600/10272 (3.9/sec)...\n",
      "  Processing 6700/10272 (3.9/sec)...\n",
      "  Processing 6800/10272 (3.9/sec)...\n",
      "  Processing 6900/10272 (3.9/sec)...\n",
      "  Processing 7000/10272 (3.9/sec)...\n",
      "  Processing 7100/10272 (3.9/sec)...\n",
      "  Processing 7200/10272 (3.9/sec)...\n",
      "  Processing 7300/10272 (3.9/sec)...\n",
      "  Processing 7400/10272 (3.9/sec)...\n",
      "  Processing 7500/10272 (3.9/sec)...\n",
      "  Processing 7600/10272 (3.9/sec)...\n",
      "  Processing 7700/10272 (3.9/sec)...\n",
      "  Processing 7800/10272 (3.9/sec)...\n",
      "  Processing 7900/10272 (4.0/sec)...\n",
      "  Processing 8000/10272 (4.0/sec)...\n",
      "  Processing 8100/10272 (4.0/sec)...\n",
      "  Processing 8200/10272 (4.0/sec)...\n",
      "  Processing 8300/10272 (4.0/sec)...\n",
      "  Processing 8400/10272 (4.0/sec)...\n",
      "  Processing 8500/10272 (4.1/sec)...\n",
      "  Processing 8600/10272 (4.1/sec)...\n",
      "  Processing 8700/10272 (4.1/sec)...\n",
      "  Processing 8800/10272 (4.1/sec)...\n",
      "  Processing 8900/10272 (4.1/sec)...\n",
      "  Processing 9000/10272 (4.1/sec)...\n",
      "  Processing 9100/10272 (4.1/sec)...\n",
      "  Processing 9200/10272 (4.2/sec)...\n",
      "  Processing 9300/10272 (4.2/sec)...\n",
      "  Processing 9400/10272 (4.2/sec)...\n",
      "  Processing 9500/10272 (4.2/sec)...\n",
      "  Processing 9600/10272 (4.2/sec)...\n",
      "  Processing 9700/10272 (4.2/sec)...\n",
      "  Processing 9800/10272 (4.2/sec)...\n",
      "  Processing 9900/10272 (4.2/sec)...\n",
      "  Processing 10000/10272 (4.3/sec)...\n",
      "  Processing 10100/10272 (4.3/sec)...\n",
      "  Processing 10200/10272 (4.3/sec)...\n",
      "----------------------------------------------------------------------\n",
      "Generated 9575 embeddings in 2394.7s\n",
      "Skipped 697 mailboxes (< 200 chars after cleaning)\n",
      "Rate: 4.0 embeddings/sec\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3 (FIXED): Generate STAR embeddings - TOP MESSAGE ONLY\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "def extract_email_body_v3(thread_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract ONLY the first/top message - the actual authored content.\n",
    "    STOPS (breaks) at ANY sign of quoted/forwarded content.\n",
    "    \"\"\"\n",
    "    if not thread_text:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = thread_text.split('\\n')\n",
    "    \n",
    "    # Step 1: Skip past all headers (find body start)\n",
    "    body_start = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('X-FileName:'):\n",
    "            body_start = i + 1\n",
    "            break\n",
    "        if line.strip() == '' and i > 0:\n",
    "            prev_lines = lines[max(0,i-3):i]\n",
    "            if any(':' in l and l.split(':')[0].replace('-','').replace('_','').isalnum() for l in prev_lines):\n",
    "                body_start = i + 1\n",
    "                break\n",
    "    \n",
    "    # Step 2: Collect lines until we hit ANY reply/forward indicator\n",
    "    body_lines = []\n",
    "    for line in lines[body_start:]:\n",
    "        \n",
    "        # STOP at quoted lines\n",
    "        if line.strip().startswith('>'):\n",
    "            break\n",
    "        \n",
    "        # STOP at \">>> email@domain\" reply markers\n",
    "        if re.search(r'>{2,}.*@.*>{2,}', line):\n",
    "            break\n",
    "        if re.search(r'>>>.*@', line):\n",
    "            break\n",
    "            \n",
    "        # STOP at \"On DATE, NAME wrote:\" \n",
    "        if re.match(r'^\\s*On .+wrote:\\s*$', line, re.IGNORECASE):\n",
    "            break\n",
    "            \n",
    "        # STOP at forwarding markers\n",
    "        if re.match(r'^\\s*-{3,}.*(Original|Forwarded).*-*\\s*$', line, re.IGNORECASE):\n",
    "            break\n",
    "        if '---------------------- Forwarded by' in line:\n",
    "            break\n",
    "        if '----- Forwarded by' in line:\n",
    "            break\n",
    "        if '-----Original Message-----' in line:\n",
    "            break\n",
    "            \n",
    "        # STOP at inline \"From:\" headers (forwarded content)\n",
    "        if re.match(r'^\\s*From:\\s*[\\w\\s]*<?[\\w\\.-]+@[\\w\\.-]+>?\\s*$', line):\n",
    "            break\n",
    "            \n",
    "        # STOP at \"Sent:\" header (often follows From: in forwards)\n",
    "        if re.match(r'^\\s*Sent:\\s+', line):\n",
    "            break\n",
    "        \n",
    "        body_lines.append(line)\n",
    "    \n",
    "    body_text = '\\n'.join(body_lines)\n",
    "    \n",
    "    # Step 3: Remove signature artifacts (phone numbers at end)\n",
    "    body_text = re.sub(r'\\n\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\s*$', '', body_text)\n",
    "    \n",
    "    # Step 4: Clean up whitespace\n",
    "    body_text = re.sub(r'\\n{3,}', '\\n\\n', body_text)\n",
    "    body_text = body_text.strip()\n",
    "    \n",
    "    return body_text\n",
    "\n",
    "\n",
    "def concatenate_emails_v3(threads: list, max_chars: int = 15000) -> str:\n",
    "    \"\"\"Concatenate unique TOP-LEVEL email bodies only.\"\"\"\n",
    "    if not threads:\n",
    "        return \"\"\n",
    "    \n",
    "    seen_content = set()\n",
    "    segments = []\n",
    "    total_chars = 0\n",
    "    \n",
    "    for thread in threads:\n",
    "        if thread is None:\n",
    "            continue\n",
    "        \n",
    "        segment = extract_email_body_v3(thread)\n",
    "        \n",
    "        # Skip very short segments\n",
    "        if len(segment) < 50:\n",
    "            continue\n",
    "        \n",
    "        # Skip attachment-only messages\n",
    "        if segment.count('.doc') + segment.count('.xls') + segment.count('.pdf') > len(segment) / 100:\n",
    "            continue\n",
    "        \n",
    "        # Deduplicate\n",
    "        fingerprint = segment[:100].strip().lower()\n",
    "        if fingerprint in seen_content:\n",
    "            continue\n",
    "        seen_content.add(fingerprint)\n",
    "        \n",
    "        segments.append(segment)\n",
    "        total_chars += len(segment)\n",
    "        \n",
    "        if total_chars > max_chars:\n",
    "            break\n",
    "    \n",
    "    return \"\\n\\n\".join(segments)\n",
    "\n",
    "\n",
    "# Process all mailboxes\n",
    "MIN_TEXT_LENGTH = 200\n",
    "embeddings_data_v3 = []\n",
    "skipped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Generating STAR embeddings for {len(mailbox_data)} mailboxes (TOP MESSAGE ONLY)...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in mailbox_data.iterrows():\n",
    "    if idx % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = idx / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Processing {idx}/{len(mailbox_data)} ({rate:.1f}/sec)...\")\n",
    "    \n",
    "    combined_text = concatenate_emails_v3(row['threads'])\n",
    "    \n",
    "    if len(combined_text) < MIN_TEXT_LENGTH:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    embedding = get_style_embedding(combined_text)\n",
    "    \n",
    "    if embedding:\n",
    "        embeddings_data_v3.append({\n",
    "            'mailbox': row['mailbox'],\n",
    "            'email_count': row['email_count'],\n",
    "            'text_length': len(combined_text),\n",
    "            'embedding': embedding,\n",
    "            'associated_users': row['associated_users'],\n",
    "            'communities': row['communities']\n",
    "        })\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"-\" * 70)\n",
    "print(f\"Generated {len(embeddings_data_v3)} embeddings in {elapsed:.1f}s\")\n",
    "print(f\"Skipped {skipped} mailboxes (< {MIN_TEXT_LENGTH} chars after cleaning)\")\n",
    "print(f\"Rate: {len(embeddings_data_v3)/elapsed:.1f} embeddings/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "796fb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Style nodes to Neo4j...\n",
      "  Written 100 Style nodes...\n",
      "  Written 1100 Style nodes...\n",
      "  Written 2100 Style nodes...\n",
      "  Written 3100 Style nodes...\n",
      "  Written 4100 Style nodes...\n",
      "  Written 5100 Style nodes...\n",
      "  Written 6100 Style nodes...\n",
      "  Written 7100 Style nodes...\n",
      "  Written 8100 Style nodes...\n",
      "  Written 9100 Style nodes...\n",
      "Total Style nodes created: 9575\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Write embeddings to Neo4j as Style nodes\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Writing Style nodes to Neo4j...\")\n",
    "\n",
    "# Create constraint for Style nodes\n",
    "gds.run_cypher(\"\"\"\n",
    "CREATE CONSTRAINT style_mailbox IF NOT EXISTS\n",
    "FOR (s:Style) REQUIRE s.mailbox IS UNIQUE\n",
    "\"\"\")\n",
    "\n",
    "# Batch write embeddings\n",
    "BATCH_SIZE = 100\n",
    "total_written = 0\n",
    "\n",
    "for i in range(0, len(embeddings_data_v3), BATCH_SIZE):\n",
    "    batch = embeddings_data_v3[i:i+BATCH_SIZE]\n",
    "    \n",
    "    write_query = \"\"\"\n",
    "    UNWIND $batch AS item\n",
    "    MERGE (s:Style {address: item.mailbox})\n",
    "    SET s.embedding = item.embedding,\n",
    "        s.email_count = item.email_count,\n",
    "        s.text_length = item.text_length\n",
    "    WITH s, item\n",
    "    MATCH (m:Mailbox {address: item.mailbox})\n",
    "    MERGE (m)-[:HAS_STYLE]->(s)\n",
    "    RETURN count(s) AS written\n",
    "    \"\"\"\n",
    "    \n",
    "    result = gds.run_cypher(write_query, params={'batch': batch})\n",
    "    total_written += result['written'].iloc[0]\n",
    "    \n",
    "    if (i // BATCH_SIZE) % 10 == 0:\n",
    "        print(f\"  Written {total_written} Style nodes...\")\n",
    "\n",
    "print(f\"Total Style nodes created: {total_written}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cea9759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing 'style-similarity' projection\n",
      "Creating Cypher projection of Style nodes...\n",
      "| graphName        |   nodeCount |   relationshipCount |\n",
      "|:-----------------|------------:|--------------------:|\n",
      "| style-similarity |       10023 |                   0 |\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Project Style nodes with embeddings using Cypher projection\n",
    "# =============================================================================\n",
    "\n",
    "# Drop existing projection if exists\n",
    "try:\n",
    "    gds.run_cypher(\"CALL gds.graph.drop('style-similarity')\")\n",
    "    print(\"Dropped existing 'style-similarity' projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cypher projection for Style nodes with embeddings\n",
    "print(\"Creating Cypher projection of Style nodes...\")\n",
    "\n",
    "projection_result = gds.run_cypher(\"\"\"\n",
    "MATCH (s:Style)\n",
    "WHERE s.embedding IS NOT NULL\n",
    "WITH gds.graph.project(\n",
    "    'style-similarity',\n",
    "    s,\n",
    "    null,\n",
    "    {\n",
    "        sourceNodeProperties: {embedding: s.embedding},\n",
    "        targetNodeProperties: {embedding: null}\n",
    "    }\n",
    ") AS g\n",
    "RETURN g.graphName AS graphName, \n",
    "       g.nodeCount AS nodeCount, \n",
    "       g.relationshipCount AS relationshipCount\n",
    "\"\"\")\n",
    "\n",
    "print(projection_result.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e209f3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KNN to find similar writing styles...\n",
      "|   nodesCompared |   relationshipsWritten |   min_similarity |   mean_similarity |   max_similarity |   median_similarity |   p90_similarity |\n",
      "|----------------:|-----------------------:|-----------------:|------------------:|-----------------:|--------------------:|-----------------:|\n",
      "|           10023 |                  20046 |           0.8007 |            0.9366 |                1 |              0.9408 |           0.9661 |\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 6: Run KNN to create similarity relationships (mutate)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Running KNN to find similar writing styles...\")\n",
    "\n",
    "knn_result = gds.run_cypher(\"\"\"\n",
    "CALL gds.knn.mutate(\n",
    "    'style-similarity',\n",
    "    {\n",
    "        nodeProperties: ['embedding'],\n",
    "        topK: 2,\n",
    "        sampleRate: 1.0,\n",
    "        deltaThreshold: 0.001,\n",
    "        similarityCutoff: 0.6,\n",
    "        mutateRelationshipType: 'SIMILAR_STYLE',\n",
    "        mutateProperty: 'score'\n",
    "    }\n",
    ")\n",
    "YIELD nodesCompared, relationshipsWritten, similarityDistribution\n",
    "RETURN \n",
    "    nodesCompared,\n",
    "    relationshipsWritten,\n",
    "    round(similarityDistribution.min, 4) AS min_similarity,\n",
    "    round(similarityDistribution.mean, 4) AS mean_similarity,\n",
    "    round(similarityDistribution.max, 4) AS max_similarity,\n",
    "    round(similarityDistribution.p50, 4) AS median_similarity,\n",
    "    round(similarityDistribution.p90, 4) AS p90_similarity\n",
    "\"\"\")\n",
    "\n",
    "print(knn_result.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a0e5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\"\"\"\n",
    "CALL gds.graph.relationships.toUndirected(\n",
    "  'style-similarity',\n",
    "  {relationshipType: 'SIMILAR_STYLE', mutateRelationshipType: 'STYLE_UNDIRECTED'}\n",
    ")\n",
    "YIELD\n",
    "  inputRelationships, relationshipsWritten\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a96552aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Leiden community detection (stats mode for tuning)...\n",
      "\n",
      "Leiden Stats by Gamma (resolution):\n",
      "|   gamma |   communityCount |   modularity |   ranLevels |\n",
      "|--------:|-----------------:|-------------:|------------:|\n",
      "|     0.1 |               16 |       0.9096 |           7 |\n",
      "|     0.5 |               35 |       0.8124 |           6 |\n",
      "|     1   |               56 |       0.7857 |           6 |\n",
      "|     2   |              105 |       0.7653 |           5 |\n",
      "|     5   |              198 |       0.7382 |           4 |\n",
      "|    10   |              323 |       0.7135 |           4 |\n",
      "|    20   |              477 |       0.6832 |           5 |\n",
      "|    50   |              801 |       0.6288 |           4 |\n",
      "|   100   |             1171 |       0.5697 |           4 |\n",
      "\n",
      "Higher gamma = more communities, Lower gamma = fewer larger communities\n",
      "Choose gamma with good modularity and reasonable community count\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 7: Run Leiden stats for tuning\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Running Leiden community detection (stats mode for tuning)...\")\n",
    "\n",
    "# Try different resolution values\n",
    "resolutions = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]\n",
    "\n",
    "leiden_stats = []\n",
    "for res in resolutions:\n",
    "    result = gds.run_cypher(f\"\"\"\n",
    "    CALL gds.leiden.stats(\n",
    "        'style-similarity',\n",
    "        {{\n",
    "            relationshipTypes: ['STYLE_UNDIRECTED'],\n",
    "            relationshipWeightProperty: 'score',\n",
    "            gamma: {res},\n",
    "            maxLevels: 10\n",
    "        }}\n",
    "    )\n",
    "    YIELD communityCount, modularity, ranLevels\n",
    "    RETURN {res} AS gamma, communityCount, round(modularity, 4) AS modularity, ranLevels\n",
    "    \"\"\")\n",
    "    leiden_stats.append(result.iloc[0].to_dict())\n",
    "\n",
    "import pandas as pd\n",
    "df_stats = pd.DataFrame(leiden_stats)\n",
    "print(\"\\nLeiden Stats by Gamma (resolution):\")\n",
    "print(df_stats.to_markdown(index=False))\n",
    "print(\"\\nHigher gamma = more communities, Lower gamma = fewer larger communities\")\n",
    "print(\"Choose gamma with good modularity and reasonable community count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f50bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Leiden with gamma=20.0 and writing to nodes...\n",
      "|   communityCount |   modularity |   ranLevels |   nodePropertiesWritten |\n",
      "|-----------------:|-------------:|------------:|------------------------:|\n",
      "|              486 |       0.6829 |           4 |                   10023 |\n",
      "\n",
      "Community Size Distribution:\n",
      "|   total_communities |   avg_size |   min_size |   max_size |   median_size |   p90_size |\n",
      "|--------------------:|-----------:|-----------:|-----------:|--------------:|-----------:|\n",
      "|                 486 |    20.6235 |          3 |         73 |            20 |         31 |\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 8: Run Leiden write with chosen gamma\n",
    "# =============================================================================\n",
    "\n",
    "# Choose gamma based on stats above (adjust as needed)\n",
    "CHOSEN_GAMMA = 20.0  # <-- Adjust based on Step 7 results\n",
    "\n",
    "print(f\"Running Leiden with gamma={CHOSEN_GAMMA} and writing to nodes...\")\n",
    "\n",
    "leiden_write = gds.run_cypher(f\"\"\"\n",
    "CALL gds.leiden.write(\n",
    "    'style-similarity',\n",
    "    {{\n",
    "        relationshipTypes: ['STYLE_UNDIRECTED'],\n",
    "        relationshipWeightProperty: 'score',\n",
    "        gamma: {CHOSEN_GAMMA},\n",
    "        maxLevels: 10,\n",
    "        writeProperty: 'styleCommunity'\n",
    "    }}\n",
    ")\n",
    "YIELD communityCount, modularity, ranLevels, nodePropertiesWritten\n",
    "RETURN communityCount, \n",
    "       round(modularity, 4) AS modularity, \n",
    "       ranLevels, \n",
    "       nodePropertiesWritten\n",
    "\"\"\")\n",
    "\n",
    "print(leiden_write.to_markdown(index=False))\n",
    "\n",
    "# Show community size distribution\n",
    "community_dist = gds.run_cypher(\"\"\"\n",
    "MATCH (s:Style)\n",
    "WHERE s.styleCommunity IS NOT NULL\n",
    "WITH s.styleCommunity AS community, count(*) AS size\n",
    "RETURN \n",
    "    count(*) AS total_communities,\n",
    "    avg(size) AS avg_size,\n",
    "    min(size) AS min_size,\n",
    "    max(size) AS max_size,\n",
    "    percentileCont(size, 0.5) AS median_size,\n",
    "    percentileCont(size, 0.9) AS p90_size\n",
    "\"\"\")\n",
    "print(\"\\nCommunity Size Distribution:\")\n",
    "print(community_dist.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b5ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
