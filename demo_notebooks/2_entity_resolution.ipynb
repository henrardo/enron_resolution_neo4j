{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bafc3a",
   "metadata": {},
   "source": [
    "# Entity Resolution with Neo4j GDS\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Completed Notebook 1** - Data must be imported into Neo4j\n",
    "2. **Neo4j running** with the `neo4j` database\n",
    "3. **GDS plugin installed** (version 2.3+)\n",
    "4. **Environment configured** - `.env` file with credentials\n",
    "\n",
    "**Expected Runtime**: 30-45 minutes for full dataset\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem\n",
    "\n",
    "The same person often appears multiple times in our data:\n",
    "- `kenneth.lay@enron.com` and `klay@enron.com` and `ken.lay@enron.com`\n",
    "- `jeff.skilling@enron.com` and `jeffrey.skilling@enron.com`\n",
    "- Display names: \"Ken Lay\" vs \"Kenneth L. Lay\" vs \"Lay, Kenneth\"\n",
    "\n",
    "**Goal**: Identify and link ~5,000 duplicate User nodes with high confidence\n",
    "\n",
    "## The Solution\n",
    "\n",
    "Multi-stage entity resolution pipeline:\n",
    "\n",
    "1. **Community Detection** (Louvain) - Partition graph into manageable communities\n",
    "2. **Node Similarity** - Find users with overlapping mailbox connections  \n",
    "3. **String Matching** (Jaro-Winkler) - Validate name similarity\n",
    "4. **Connected Components** (WCC) - Group all aliases together\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc4b1e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from graphdatascience import GraphDataScience\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "DATABASE = \"neo4j\"\n",
    "\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"NEO4J_PASSWORD not found in .env file!\")\n",
    "\n",
    "try:\n",
    "    gds = GraphDataScience(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD), database=DATABASE)\n",
    "    gds_version = gds.version()\n",
    "    print(f\"Connected to Neo4j GDS {gds_version}\")\n",
    "    \n",
    "    # Verify data exists\n",
    "    result = gds.run_cypher(\"MATCH (e:Email) RETURN count(e) as count\")\n",
    "    email_count = result['count'].iloc[0]\n",
    "    \n",
    "    if email_count == 0:\n",
    "        raise ValueError(\"No emails found! Please run Notebook 1 first to import data.\")\n",
    "    \n",
    "    print(f\"âœ“ Found {email_count:,} emails in database\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection or data validation failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Have you run Notebook 1 to import data?\")\n",
    "    print(\"  2. Is Neo4j running with the 'neo4j database?\")\n",
    "    print(\"  3. Is the GDS plugin installed and activated?\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877b3a9",
   "metadata": {},
   "source": [
    "## 2. Low-hanging Fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54228c6d",
   "metadata": {},
   "source": [
    "Currently, we have two nodes that identify individuals:\n",
    "\n",
    "1. `(:User)` nodes identify the names extracted from headers\n",
    "2. `(:Mailbox)` nodes identify emails extracted along with those names\n",
    "\n",
    "Both are connected individually to the emails they `:SENT`, `:RECEIVED`, `CC_ON`, `BCC_ON`\n",
    "\n",
    "We cannot use names as primary identifiers. Here's an example as to why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_people = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE toLower(u.nameRaw) CONTAINS(toLower(\"Ken\")) AND toLower(u.nameRaw) CONTAINS(toLower(\"Lay\"))\n",
    "    RETURN u.nameRaw AS name, u.primaryEmail AS email\n",
    "    ORDER BY name\n",
    "\"\"\")\n",
    "\n",
    "print(\"Ken Lay lookalikes:\")\n",
    "print(\"=\"*60)\n",
    "display(similar_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0046ec",
   "metadata": {},
   "source": [
    "These are all the emails that could refer to 'Kenneth Lay'.\n",
    "\n",
    "However, check out line 26. (It may differ slightly on yours).\n",
    "\n",
    "Ken Slay '80 is not the same person as Kenneth Lay -- yet a straight name comparison may well put them together. Jaro-Winkler would probably pass, but the point stands -- we need to do some filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e1d7f",
   "metadata": {},
   "source": [
    "### Using stable identifiers\n",
    "\n",
    "There are many ways of doing entity resolution, and the simplest and most predictable is to rely on stable, unique identifiers shared by many entities.\n",
    "\n",
    "We can then recursively tighten the graph, so that newly combined entities inherit each others' connections.\n",
    "\n",
    "In our dataset, the most stable identifiers we have are emails. \n",
    "\n",
    "When we imported, we ensured that identical email addresses could only enter the database once. So, if:\n",
    "\n",
    "klay@  got imported with the display name 'Ken Lay' and then the same email again with 'Kenneth Lay' -- they both get connected to each other intermediately via USED.\n",
    "\n",
    "Let's take a look at some of these entity pairs.\n",
    "\n",
    "Note: We'll filter out any mailboxes that have over 50 shared users -- the probability of someone having 50 ways to present their name is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75008f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show full content\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.width', None)         # Don't wrap to fit console width\n",
    "pd.set_option('display.max_seq_items', None) # Show all items in lists\n",
    "\n",
    "# Which users are connected to the same mailbox address, but have different nodes?\n",
    "same_mailbox = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)-[:USED]->(m:Mailbox)<-[:USED]-(u2:User) \n",
    "    WHERE u <> u2 \n",
    "    AND m.address CONTAINS \"@enron.com\"\n",
    "    WITH DISTINCT m.address AS address, collect(u.nameRaw) as names1, collect(u2.nameRaw) AS names2\n",
    "    WHERE size(names1) < 50 AND size(names2) < 50\n",
    "    RETURN address, names1, names2\n",
    "    ORDER BY address ASC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Which users are connected to the same mailbox address, but have different nodes?\")\n",
    "print(\"=\"*60)\n",
    "display(same_mailbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d06c3d",
   "metadata": {},
   "source": [
    "You will notice these are  -- for the most part -- correct. However, we also need to filter out generic emails like 'list' and 'no.user', etc.\n",
    "\n",
    "A super interesting example here is that 'Greg Whalley' and 'Lawrence Whalley' both share the same email address. This must be an error, right? \n",
    "\n",
    "Actually Lawrence Whalley also goes by Lawrence \"Greg\" Whalley -- hence why his name shows up in both formats. So these two should merge. On the other hand, we have examples like Randy Young and Becky Young -- the \"Randy\" seems to come from messy data, and there's not a whole lot we can do about that at this stage.\n",
    "\n",
    "You could manually attack each of these email addressess with a large dict or array -- or we can take a look at the kinds of relational attributes they display.\n",
    "\n",
    "For instance, we have already filtered the initial list to exclude `Mailbox` nodes that are connected to more than 50 names. However, we can go one step further. Let's get the degree count for every User -> Email node and append it.\n",
    "\n",
    "To do so, we:\n",
    "\n",
    "1. Project a graph of User->Mailbox via email.\n",
    "2. Run degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96880b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('user_email_bipartite')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create bipartite projection: Person -> Email\n",
    "# This connects people to the emails they participated in\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)\n",
    "    WHERE \"User\" IN labels(source)\n",
    "    OPTIONAL MATCH (source)-[r:USED]->(target:Mailbox)\n",
    "    WITH gds.graph.project(\n",
    "        'user_email_bipartite',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created bipartite projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ccb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Running Degree Centrality...\\n\")\n",
    "\n",
    "G = gds.graph.get('user_email_bipartite')\n",
    "\n",
    "# Run Degree Centrality using the GDS client\n",
    "result = gds.degree.write(\n",
    "    G,\n",
    "    writeProperty='email_degree',\n",
    "    orientation= 'REVERSE' \n",
    ")\n",
    "\n",
    "print(f\"Degree Centrality completed successfully!\")\n",
    "print(f\"Properties written: {result['nodePropertiesWritten']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1630bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_degree = gds.run_cypher(\"\"\"\n",
    "MATCH (m:Mailbox)\n",
    "WHERE m.email_degree IS NOT NULL\n",
    "OPTIONAL MATCH (u:User)-[:USED]->(m)\n",
    "WITH m, collect(DISTINCT u.nameNormalized) as owners\n",
    "RETURN \n",
    "  m.address as email,\n",
    "  owners[0..6] as owner_names,\n",
    "  size(owners) as unique_owners,\n",
    "  m.email_degree\n",
    "ORDER BY m.email_degree DESC\n",
    "LIMIT 50\n",
    "\"\"\")\n",
    "display(top_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the graph\n",
    "gds.graph.drop('user_email_bipartite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10c20c",
   "metadata": {},
   "source": [
    "From these results, we can see that everything above 19 is absolute trash in terms of entity resolution.\n",
    "\n",
    "Below 19, we start to get some decent matches, but we also have some spam.\n",
    "\n",
    "Let's run our original @enron.com focused query -- this time with the degree centrality flag set to <= 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7822f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show full content\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.width', None)         # Don't wrap to fit console width\n",
    "pd.set_option('display.max_seq_items', None) # Show all items in lists\n",
    "\n",
    "# Which users are connected to the same mailbox address, but have different nodes?\n",
    "same_mailbox = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)-[:USED]->(m:Mailbox)<-[:USED]-(u2:User) \n",
    "    WHERE u <> u2 \n",
    "    AND m.address CONTAINS \"@enron.com\"\n",
    "    AND m.email_degree <= 19\n",
    "    WITH DISTINCT m.address AS address, collect(u.nameRaw) as names1, collect(u2.nameRaw) AS names2\n",
    "    WHERE size(names1) < 50 AND size(names2) < 50\n",
    "    RETURN address, names1, names2\n",
    "    ORDER BY address ASC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Which users are connected to the same mailbox address, but have different nodes?\")\n",
    "print(\"=\"*60)\n",
    "display(same_mailbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434ef80",
   "metadata": {},
   "source": [
    "This is now a much more realistic set. There are likely still a few spurious links in here -- but it's enough to get started with.\n",
    "\n",
    "There are a few things we can do now, including:\n",
    "\n",
    "1. **Weakly Connected Components:** identifies disconnected components in the graph. You can use this to narrow down the comparison areas.\n",
    "2. **Louvain/Leiden:** Same as WCC, but they require more compute and can identify communities, even where they are connected to outside communities\n",
    "3. **Node Similarity:** To check the overlap of nodes and their neighbours\n",
    "4. **FastRP + KNN:** To get node positions and compare (can also use properties as features)\n",
    "\n",
    "There are some other methods too, but let's start with these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89236",
   "metadata": {},
   "source": [
    "## 3. WCC\n",
    "\n",
    "We'll run Weakly Connected Components to see if we have any large, separate components that we can investigate separately.\n",
    "\n",
    "First, let's just project the entire graph. \n",
    "\n",
    "Note: I am using Cypher projection here -- you can also use Native projection if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13853257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('wcc_graph')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WITH gds.graph.project(\n",
    "        'wcc_graph',\n",
    "        source,\n",
    "        target\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created wcc projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17066979",
   "metadata": {},
   "source": [
    "Next we run WCC on the graph.\n",
    "\n",
    "Before we go ahead and write results, it's worth checking whether there are any orphaned components with **stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f63dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Running WCC...\\n\")\n",
    "\n",
    "G = gds.graph.get('wcc_graph')\n",
    "\n",
    "# Run Degree Centrality using the GDS client\n",
    "result = gds.wcc.stats(\n",
    "    G\n",
    ")\n",
    "\n",
    "print(f\"WCC completed successfully!\")\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Running WCC...\\n\")\n",
    "\n",
    "G = gds.graph.get('wcc_graph')\n",
    "\n",
    "# Run Degree Centrality using the GDS client\n",
    "result = gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty='wcc_id',\n",
    "    minComponentSize=10\n",
    ")\n",
    "\n",
    "print(f\"WCC completed successfully!\")\n",
    "print(f\"Properties written: {result['nodePropertiesWritten']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show full content\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.width', None)         # Don't wrap to fit console width\n",
    "pd.set_option('display.max_seq_items', None) # Show all items in lists\n",
    "\n",
    "# Which users are connected to the same mailbox address, but have different nodes?\n",
    "nodes_per_wcc = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.wcc_id IS NOT NULL\n",
    "    WITH DISTINCT u.wcc_id AS component, collect(u) AS total_per_wcc\n",
    "    WITH component, size(total_per_wcc) AS count\n",
    "    RETURN component, count\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Let's see how many nodes are in each component...\")\n",
    "print(\"=\"*60)\n",
    "display(nodes_per_wcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dc3af",
   "metadata": {},
   "source": [
    "So, we've got many singleton users, trapped alone in their components, a few slightly larger ones, and one gigantic component, helpfully labeled 0.\n",
    "\n",
    "Running Jaro-Winkler on that would be heavy, so let's break it up some more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312a40d",
   "metadata": {},
   "source": [
    "## 4. Louvain\n",
    "\n",
    "Let's use Louvain, just on that biggest component and see if we can break it up some more.\n",
    "\n",
    "If you're not sure how Louvain works, check out the [Louvain docs](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('giant_component_graph')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Project only the giant component (wcc_id = 0)\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WHERE source.wcc_id = 0 AND target.wcc_id = 0\n",
    "    WITH gds.graph.project(\n",
    "        'giant_component_graph',\n",
    "        source,\n",
    "        target,\n",
    "        {},\n",
    "        {undirectedRelationshipTypes: ['*']}\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created giant component projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250583cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Louvain community detection\n",
    "print(\"Running Louvain community detection...\\n\")\n",
    "\n",
    "G = gds.graph.get('giant_component_graph')\n",
    "\n",
    "result = gds.louvain.write(\n",
    "    G,\n",
    "    writeProperty='louvain_community',\n",
    "    maxLevels=10,\n",
    "    includeIntermediateCommunities=False\n",
    ")\n",
    "\n",
    "print(f\"Louvain completed successfully!\")\n",
    "print(f\"Properties written: {result['nodePropertiesWritten']:,}\")\n",
    "print(f\"Communities found: {result['communityCount']:,}\")\n",
    "print(f\"Modularity: {result['modularity']:.4f}\")\n",
    "print(f\"Levels: {result['ranLevels']}\")\n",
    "\n",
    "# Display community distribution\n",
    "pd.DataFrame([result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9984984",
   "metadata": {},
   "source": [
    "On my run of this, Louvain has returned 262 communities with an overall modularity of 0.72 -- that's quite high.\n",
    "\n",
    "With such a high modularity, we can assume that most of these communities are relatively insular, rarely speaking to each other outside of their social groups.\n",
    "\n",
    "Let's get another distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of community sizes\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "community_sizes = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.louvain_community IS NOT NULL\n",
    "    WITH u.louvain_community AS community, count(*) AS size\n",
    "    RETURN community, size\n",
    "    ORDER BY size DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Found {len(community_sizes)} Louvain communities\")\n",
    "print(\"\\nTop 20 largest communities:\")\n",
    "display(community_sizes.head(20))\n",
    "\n",
    "print(f\"\\nCommunity size statistics:\")\n",
    "print(f\"  Min: {community_sizes['size'].min()}\")\n",
    "print(f\"  Max: {community_sizes['size'].max()}\")\n",
    "print(f\"  Mean: {community_sizes['size'].mean():.2f}\")\n",
    "print(f\"  Median: {community_sizes['size'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the graph\n",
    "gds.graph.drop('giant_component_graph')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e628b",
   "metadata": {},
   "source": [
    "That's a pretty big difference -- we've now got a relatively even spread of communities across the board.\n",
    "\n",
    "This is still quite a lot of people to run Jaro-Winkler on -- it is quadratic, so every node will have to be checked against every other node. \n",
    "\n",
    "Still, for the hell of it, let's test on one community -- the largest -- just to see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfa46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest community for testing\n",
    "largest_community = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.louvain_community IS NOT NULL\n",
    "    WITH u.louvain_community AS community, count(*) AS size\n",
    "    RETURN community, size\n",
    "    ORDER BY size DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "community_id = largest_community['community'].iloc[0]\n",
    "community_size = largest_community['size'].iloc[0]\n",
    "\n",
    "print(f\"Processing community {community_id} (size: {community_size})...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "matches = gds.run_cypher(\"\"\"\n",
    "    MATCH (u1:User)\n",
    "    WHERE u1.louvain_community = $community_id\n",
    "      AND u1.nameNormalized IS NOT NULL\n",
    "      AND u1.nameNormalized <> ''\n",
    "    \n",
    "    CALL (u1) {\n",
    "        WITH u1\n",
    "        MATCH (u2:User)\n",
    "        WHERE u2.louvain_community = $community_id\n",
    "          AND u2.nameNormalized IS NOT NULL\n",
    "          AND u2.nameNormalized <> ''\n",
    "          AND u1 < u2\n",
    "          AND substring(toLower(u1.nameNormalized), 0, 1) = substring(toLower(u2.nameNormalized), 0, 1)\n",
    "        \n",
    "        WITH u1, u2,\n",
    "             apoc.text.jaroWinklerDistance(\n",
    "                 toLower(u1.nameNormalized), \n",
    "                 toLower(u2.nameNormalized)\n",
    "             ) AS distance\n",
    "        \n",
    "        WHERE distance <= 0.08\n",
    "        \n",
    "        RETURN u1.nameRaw AS name1,\n",
    "               u2.nameRaw AS name2,\n",
    "               u1.nameNormalized AS norm1,\n",
    "               u2.nameNormalized AS norm2,\n",
    "               round(distance * 1000) / 1000.0 AS jw_distance,\n",
    "               round((1 - distance) * 1000) / 1000.0 AS similarity_score\n",
    "    } IN TRANSACTIONS OF 100 ROWS\n",
    "    \n",
    "    RETURN $community_id AS community,\n",
    "           name1, name2, norm1, norm2, jw_distance, similarity_score\n",
    "    ORDER BY jw_distance DESC\n",
    "\"\"\", params={'community_id': community_id})\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(matches)} potential matches\")\n",
    "\n",
    "if len(matches) > 0:\n",
    "    print(f\"\\nMatch statistics:\")\n",
    "    print(f\"  Min distance: {matches['jw_distance'].min():.3f}\")\n",
    "    print(f\"  Max distance: {matches['jw_distance'].max():.3f}\")\n",
    "    print(f\"  Mean distance: {matches['jw_distance'].mean():.3f}\")\n",
    "    \n",
    "    display(matches.head(50))\n",
    "else:\n",
    "    print(\"No matches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef4fbb",
   "metadata": {},
   "source": [
    "We've got some pretty good matches here, but still have some poor samples pushing through.\n",
    "\n",
    "Many of the issues stem from the failure to strip out the '(E-mail)' and '\\(E-mail)\\' tags in the name headers.\n",
    "\n",
    "You can fix things like this on import, or wait until they appear in your resolution work. I prefer the latter for two reasons:\n",
    "\n",
    "1. It reminds you that the data is dirty -- overconfidence can lead spurious resolutions\n",
    "2. Sometimes, those things you strip in the import turn out to be important later on. For instance, it might turn out that something as innocuous as '(E-mail)' can be used to identify different email clients sent to or from.\n",
    "\n",
    "Let's make a new property, nameNormStrip where we remove all of the mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d762555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nameNormStrip property by removing email suffixes\n",
    "print(\"Creating nameNormStrip property...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "result = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.nameNormalized IS NOT NULL\n",
    "    \n",
    "    CALL (u) {\n",
    "        WITH u\n",
    "        WITH u,\n",
    "             replace(\n",
    "                 replace(\n",
    "                     replace(\n",
    "                         replace(u.nameNormalized, '\\\\(E-Mail\\\\)', ''),\n",
    "                         '(E-Mail)', ''\n",
    "                     ),\n",
    "                     '(E-Mail 2)', ''\n",
    "                 ),\n",
    "                 '(Personal)', ''\n",
    "             ) AS stripped\n",
    "        SET u.nameNormStrip = trim(stripped)\n",
    "    } IN TRANSACTIONS OF 1000 ROWS\n",
    "    \n",
    "    RETURN count(*) AS nodes_updated\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710768c",
   "metadata": {},
   "source": [
    "And let's run it again using the newly stripped property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce73c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest community for testing\n",
    "largest_community = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.louvain_community IS NOT NULL\n",
    "    WITH u.louvain_community AS community, count(*) AS size\n",
    "    RETURN community, size\n",
    "    ORDER BY size DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "community_id = largest_community['community'].iloc[0]\n",
    "community_size = largest_community['size'].iloc[0]\n",
    "\n",
    "print(f\"Testing with largest community: {community_id} (size: {community_size})\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "matches = gds.run_cypher(\"\"\"\n",
    "    MATCH (u1:User)\n",
    "    WHERE u1.louvain_community = $community_id\n",
    "      AND u1.nameNormStrip IS NOT NULL\n",
    "      AND u1.nameNormStrip <> ''\n",
    "    \n",
    "    CALL (u1) {\n",
    "        WITH u1\n",
    "        MATCH (u2:User)\n",
    "        WHERE u2.louvain_community = $community_id\n",
    "          AND u2.nameNormStrip IS NOT NULL\n",
    "          AND u2.nameNormStrip <> ''\n",
    "          AND u1 < u2\n",
    "          AND substring(toLower(u1.nameNormStrip), 0, 1) = substring(toLower(u2.nameNormStrip), 0, 1)\n",
    "        \n",
    "        WITH u1, u2,\n",
    "             apoc.text.jaroWinklerDistance(\n",
    "                 toLower(u1.nameNormStrip), \n",
    "                 toLower(u2.nameNormStrip)\n",
    "             ) AS distance\n",
    "        \n",
    "        WHERE distance <= 0.08\n",
    "        \n",
    "        RETURN u1.nameRaw AS name1,\n",
    "               u2.nameRaw AS name2,\n",
    "               u1.nameNormStrip AS normStrip1,\n",
    "               u2.nameNormStrip AS normStrip2,\n",
    "               round(distance * 1000) / 1000.0 AS jw_distance,\n",
    "               round((1 - distance) * 1000) / 1000.0 AS similarity_score\n",
    "    } IN TRANSACTIONS OF 100 ROWS\n",
    "    \n",
    "    RETURN $community_id AS community,\n",
    "           name1, name2, normStrip1, normStrip2, jw_distance, similarity_score\n",
    "    ORDER BY jw_distance DESC\n",
    "\"\"\", params={'community_id': community_id})\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(matches)} potential matches\")\n",
    "\n",
    "if len(matches) > 0:\n",
    "    print(f\"\\nMatch statistics:\")\n",
    "    print(f\"  Min distance: {matches['jw_distance'].min():.3f}\")\n",
    "    print(f\"  Max distance: {matches['jw_distance'].max():.3f}\")\n",
    "    print(f\"  Mean distance: {matches['jw_distance'].mean():.3f}\")\n",
    "    \n",
    "    display(matches.head(50))\n",
    "else:\n",
    "    print(\"No matches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab068e5",
   "metadata": {},
   "source": [
    "At this point, you may notice that 'if only we were to add more rules, we could get more names to match'. That would be a mistake, leading to hours of adding 'just one more rule'.\n",
    "\n",
    "With entity resolution, one should expect the outcome to be relatively generalisable. The idiosyncracies in one region of the graph may differ entirely from another. So, yes, add more rules, but don't go overboard.\n",
    "\n",
    "Better yet, let's further filter our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed08f",
   "metadata": {},
   "source": [
    "## 5. Node Similarity\n",
    "\n",
    "In this particular graph we don't have many identifying, unique features that can identify an individual except that they used the same mailbox.\n",
    "\n",
    "The same is true of graphs you'll work with from unstructred text.\n",
    "\n",
    "However, node similarity is still sort of useful here, as you will see.\n",
    "\n",
    "First, let's project a graph of one of our communities.\n",
    "\n",
    "We're only going to project `(User)-[:USED]->(Mailbox)` -- this gives us a bipartite graph where no users connect directly to each other, and no mailboxes do either.\n",
    "\n",
    "In this projection, Node Similarity will look for Users whose mailboxes overlap across the community. Let's run it first, before we look at why it's powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest community for testing\n",
    "largest_community = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.louvain_community IS NOT NULL\n",
    "    WITH u.louvain_community AS community, count(*) AS size\n",
    "    RETURN community, size\n",
    "    ORDER BY size DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "community_id = largest_community['community'].iloc[0]\n",
    "community_size = largest_community['size'].iloc[0]\n",
    "\n",
    "print(f\"Testing with largest community: {community_id} (size: {community_size})\")\n",
    "\n",
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('user_mailbox_similarity')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create bipartite projection: User -> Mailbox\n",
    "print(\"Creating User-Mailbox bipartite projection...\\n\")\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source:User)-[r:USED]->(target:Mailbox)\n",
    "    WHERE source.louvain_community = $community_id\n",
    "    WITH gds.graph.project(\n",
    "        'user_mailbox_similarity',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\", params={'community_id': community_id})\n",
    "\n",
    "print(\"Created bipartite projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e15e13",
   "metadata": {},
   "source": [
    "Now that we've projected it, we can run node similarity -- same as with any algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Node Similarity results\n",
    "print(\"Running Node Similarity (streaming)...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "similarity_results = gds.run_cypher(\"\"\"\n",
    "    CALL gds.nodeSimilarity.stream('user_mailbox_similarity', {\n",
    "        similarityCutoff: 0.5,\n",
    "        topK: 5,\n",
    "        similarityMetric: 'overlap'\n",
    "    })\n",
    "    YIELD node1, node2, similarity\n",
    "    WITH gds.util.asNode(node1) AS u1, gds.util.asNode(node2) AS u2, similarity\n",
    "    WHERE u1.nameNormStrip IS NOT NULL AND u2.nameNormStrip IS NOT NULL\n",
    "    RETURN u1.nameNormStrip AS name1,\n",
    "           u2.nameNormStrip AS name2,\n",
    "           u1.nameNormalized AS norm1,\n",
    "           u2.nameNormalized AS norm2,\n",
    "           round(similarity * 1000) / 1000.0 AS similarity_score\n",
    "    ORDER BY similarity DESC\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(similarity_results)} similarity pairs\")\n",
    "\n",
    "if len(similarity_results) > 0:\n",
    "    print(f\"\\nSimilarity statistics:\")\n",
    "    print(f\"  Min: {similarity_results['similarity_score'].min():.3f}\")\n",
    "    print(f\"  Max: {similarity_results['similarity_score'].max():.3f}\")\n",
    "    print(f\"  Mean: {similarity_results['similarity_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {similarity_results['similarity_score'].median():.3f}\")\n",
    "    \n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    display(similarity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d16247",
   "metadata": {},
   "source": [
    "Let's try this now on a projection of the full graph, keeping our degree centrality filter from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('user_mailbox_similarity_full')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create bipartite projection: User -> Mailbox (filtered by degree centrality)\n",
    "print(\"Creating User-Mailbox bipartite projection for entire dataset...\\n\")\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source:User)-[r:USED]->(target:Mailbox)\n",
    "    WHERE target.email_degree IS NOT NULL\n",
    "      AND target.email_degree <= 19\n",
    "    WITH gds.graph.project(\n",
    "        'user_mailbox_similarity_full',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created bipartite projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Node Similarity results for entire dataset\n",
    "print(\"Running Node Similarity (streaming) on full dataset...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "similarity_results = gds.run_cypher(\"\"\"\n",
    "    CALL gds.nodeSimilarity.stream('user_mailbox_similarity_full', {\n",
    "        similarityCutoff: 0.5,\n",
    "        topK: 5,\n",
    "        similarityMetric: 'overlap'\n",
    "    })\n",
    "    YIELD node1, node2, similarity\n",
    "    WITH gds.util.asNode(node1) AS u1, gds.util.asNode(node2) AS u2, similarity\n",
    "    WHERE u1.nameNormStrip IS NOT NULL AND u2.nameNormStrip IS NOT NULL\n",
    "    RETURN u1.nameNormStrip AS name1,\n",
    "           u2.nameNormStrip AS name2,\n",
    "           u1.nameNormalized AS norm1,\n",
    "           u2.nameNormalized AS norm2,\n",
    "           round(similarity * 1000) / 1000.0 AS similarity_score\n",
    "    ORDER BY similarity DESC\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(similarity_results)} similarity pairs\")\n",
    "\n",
    "if len(similarity_results) > 0:\n",
    "    print(f\"\\nSimilarity statistics:\")\n",
    "    print(f\"  Min: {similarity_results['similarity_score'].min():.3f}\")\n",
    "    print(f\"  Max: {similarity_results['similarity_score'].max():.3f}\")\n",
    "    print(f\"  Mean: {similarity_results['similarity_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {similarity_results['similarity_score'].median():.3f}\")\n",
    "    \n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(similarity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de759ce",
   "metadata": {},
   "source": [
    "We could accept these results and just merge the entities.\n",
    "\n",
    "There would be problems for sure, but it would be useable.\n",
    "\n",
    "Node Similarity here has checked which User nodes overlap with all other nodes in the graph in terms of shared neighbours. It has then calculated the similarity of those nodes by overlap. You could also use Jaccard or Cosine. \n",
    "\n",
    "Each of the three has its benefits:\n",
    "\n",
    "- **Jaccard:** Good as a baseline\n",
    "- **Overlap:** Good for when you have missing data\n",
    "- **Cosine:** Best for when you have complete data across the set\n",
    "\n",
    "For now, we're not going to merge these -- as there is another technique that can be of some use to you, and which will likely work better on this particular dataset. However, if you find yourself working with things like financial data, census data, etc, where users can have multiple unique identifiers and descriptors, Node Similarity can often turn out to be the simplest, most effective method of disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.drop('user_mailbox_similarity_full')\n",
    "print(\"Dropped existing projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6faf2e",
   "metadata": {},
   "source": [
    "## 6. FastRP + KNN\n",
    "\n",
    "FastRP is an algorithm that turn the topology of your nodes into a vector representation. There are a lot of ways to finesse this, and we won't go into those in detail here.\n",
    "\n",
    "You can get a deeper walkthrough of this algorithm in [this notebook exploring GDS with Python](https://github.com/henrardo/workshop-gds-repo).\n",
    "\n",
    "For now, let's just run a basic workflow, and see how we can use this to generate embeddings to encode our nodes' positions.\n",
    "\n",
    "Then, we'll run KNN to find the most similar nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0496539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('full_graph_fastrp')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create projection with ALL nodes and relationships\n",
    "print(\"Creating full graph projection for FastRP...\\n\")\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WITH gds.graph.project(\n",
    "        'full_graph_fastrp',\n",
    "        source,\n",
    "        target\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created full graph projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86795a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('full_graph_fastrp')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create projection with ALL nodes and relationships\n",
    "print(\"Creating full graph projection for FastRP...\\n\")\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WITH gds.graph.project(\n",
    "        'full_graph_fastrp',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created full graph projection:\")\n",
    "display(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastRP to generate embeddings on the full graph\n",
    "print(\"Running FastRP to generate node embeddings on full graph...\\n\")\n",
    "\n",
    "G = gds.graph.get('full_graph_fastrp')\n",
    "\n",
    "fastrp_result = gds.fastRP.write(\n",
    "    G,\n",
    "    writeProperty='fastrp_embedding_full',\n",
    "    embeddingDimension=128,\n",
    "    iterationWeights=[0.0, 0.5, 0.5, 0.5, 0.5],\n",
    "    randomSeed=42\n",
    ")\n",
    "\n",
    "print(f\"FastRP completed successfully!\")\n",
    "print(f\"Properties written: {fastrp_result['nodePropertiesWritten']:,}\")\n",
    "\n",
    "pd.DataFrame([fastrp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78db4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new projection with User nodes and their embeddings for KNN\n",
    "try:\n",
    "    gds.graph.drop('user_knn_full_graph')\n",
    "    print(\"Dropped existing KNN projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nCreating projection for KNN with embeddings from full graph...\\n\")\n",
    "\n",
    "knn_projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source:User)\n",
    "    WHERE source.fastrp_embedding_full IS NOT NULL\n",
    "    WITH gds.graph.project(\n",
    "        'user_knn_full_graph',\n",
    "        source,\n",
    "        null,\n",
    "        {\n",
    "            sourceNodeProperties: source {\n",
    "                .fastrp_embedding_full\n",
    "            },\n",
    "            targetNodeProperties: null\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created KNN projection:\")\n",
    "display(knn_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN to find similar users based on full graph embeddings\n",
    "print(\"Running KNN to find similar users based on full graph context...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "knn_results = gds.run_cypher(\"\"\"\n",
    "    CALL gds.knn.stream('user_knn_full_graph', {\n",
    "        nodeProperties: ['fastrp_embedding_full'],\n",
    "        topK: 5,\n",
    "        sampleRate: 1.0,\n",
    "        deltaThreshold: 0.0,\n",
    "        concurrency: 4,\n",
    "        similarityCutoff: 0.999\n",
    "    })\n",
    "    YIELD node1, node2, similarity\n",
    "    WITH gds.util.asNode(node1) AS u1, gds.util.asNode(node2) AS u2, similarity\n",
    "    WHERE u1.nameNormStrip IS NOT NULL \n",
    "      AND u2.nameNormStrip IS NOT NULL\n",
    "    RETURN u1.nameNormStrip AS name1,\n",
    "           u2.nameNormStrip AS name2,\n",
    "           u1.nameNormalized AS norm1,\n",
    "           u2.nameNormalized AS norm2,\n",
    "           similarity AS similarity_score\n",
    "    ORDER BY similarity DESC\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(knn_results)} similar pairs\")\n",
    "\n",
    "if len(knn_results) > 0:\n",
    "    print(f\"\\nKNN Similarity statistics:\")\n",
    "    print(f\"  Min: {knn_results['similarity_score'].min():.3f}\")\n",
    "    print(f\"  Max: {knn_results['similarity_score'].max():.3f}\")\n",
    "    print(f\"  Mean: {knn_results['similarity_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {knn_results['similarity_score'].median():.3f}\")\n",
    "    \n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    display(knn_results.head(100))\n",
    "else:\n",
    "    print(\"No similar pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ea98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "gds.graph.drop('full_graph_fastrp')\n",
    "gds.graph.drop('user_knn_full_graph')\n",
    "print(\"Dropped projections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecef7d",
   "metadata": {},
   "source": [
    "First, we'll convert our date stamps into floats (features in FastRP must always be encoded as floats or list of floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ca738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert date strings to float timestamps and store as a new property\n",
    "print(\"Step 1: Converting date strings to float timestamps...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "date_conversion = gds.run_cypher(\"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n.date IS NOT NULL\n",
    "    \n",
    "    CALL (n) {\n",
    "        WITH n\n",
    "        SET n.dateFloat = toFloat(datetime(n.date).epochMillis)\n",
    "    } IN TRANSACTIONS OF 1000 ROWS\n",
    "    \n",
    "    RETURN count(*) AS nodes_updated\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "display(date_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50f202",
   "metadata": {},
   "source": [
    "We can improve the accuracy of FastRP by including node features. This allows us to infer the attributes of nodes using one or more algorithms and then have those encoded as part of the vectors output by FastRP.\n",
    "\n",
    "In this case, let's get degree centrality for every node in the graph. This should naturally separate generic, high-volume email nodes from specific, lower volume user nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07336b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create standard graph projection and run degree centrality\n",
    "try:\n",
    "    gds.graph.drop('full_graph_for_degree')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nStep 2: Creating full graph projection for degree centrality...\\n\")\n",
    "\n",
    "degree_projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WITH gds.graph.project(\n",
    "        'full_graph_for_degree',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        },\n",
    "        {\n",
    "            undirectedRelationshipTypes: ['*']\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created projection for degree centrality:\")\n",
    "display(degree_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb353725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Degree Centrality (undirected) and write to graph\n",
    "print(\"Running Degree Centrality (undirected)...\\n\")\n",
    "\n",
    "G = gds.graph.get('full_graph_for_degree')\n",
    "\n",
    "degree_result = gds.degree.write(\n",
    "    G,\n",
    "    writeProperty='degree_centrality_undirected',\n",
    "    orientation='UNDIRECTED'\n",
    ")\n",
    "\n",
    "print(f\"Degree Centrality completed successfully!\")\n",
    "print(f\"Properties written: {degree_result['nodePropertiesWritten']:,}\")\n",
    "\n",
    "pd.DataFrame([degree_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b24542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up degree projection\n",
    "gds.graph.drop('full_graph_for_degree')\n",
    "print(\"Dropped degree projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b55b2",
   "metadata": {},
   "source": [
    "We could add these nodeProperties to our graph as is, but we'll end up with better results if we scale them first. So, let's now project a new graph to create scaled Properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78257067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create projection with properties for scaling\n",
    "try:\n",
    "    gds.graph.drop('graph_for_scaling')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nStep 3: Creating projection for scaling properties...\\n\")\n",
    "\n",
    "scaling_projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (n)\n",
    "    RETURN gds.graph.project(\n",
    "        'graph_for_scaling',\n",
    "        n,\n",
    "        null,\n",
    "        {\n",
    "            sourceNodeProperties: n { \n",
    "                dateFloat: coalesce(n.dateFloat, 0.0),\n",
    "                degree_centrality_undirected: coalesce(n.degree_centrality_undirected, 0.0)\n",
    "            },\n",
    "            targetNodeProperties: {}\n",
    "        }\n",
    "    ) AS g\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created projection for scaling:\")\n",
    "display(scaling_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa76dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Scale properties using GDS Scale Properties algorithm\n",
    "print(\"\\nStep 4: Scaling properties using MinMax scaler...\\n\")\n",
    "\n",
    "G_scale = gds.graph.get('graph_for_scaling')\n",
    "\n",
    "scale_result = gds.scaleProperties.mutate(\n",
    "    G_scale,\n",
    "    nodeProperties=['degree_centrality_undirected', 'dateFloat'],\n",
    "    scaler='MinMax',\n",
    "    mutateProperty='scaledFeatures'\n",
    ")\n",
    "\n",
    "print(f\"Scale Properties completed successfully!\")\n",
    "print(f\"Properties mutated: {scale_result['nodePropertiesWritten']:,}\")\n",
    "\n",
    "pd.DataFrame([scale_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Write the scaled properties back to Neo4j\n",
    "print(\"\\nStep 5: Writing scaled properties back to Neo4j...\\n\")\n",
    "\n",
    "write_scaled = gds.run_cypher(\"\"\"\n",
    "    CALL gds.graph.nodeProperty.stream('graph_for_scaling', 'scaledFeatures')\n",
    "    YIELD nodeId, propertyValue\n",
    "    WITH gds.util.asNode(nodeId) AS node, propertyValue\n",
    "    SET node.scaledFeatures = propertyValue\n",
    "    RETURN count(*) AS nodes_updated\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote scaled properties:\")\n",
    "display(write_scaled)\n",
    "\n",
    "# Clean up scaling projection\n",
    "gds.graph.drop('graph_for_scaling')\n",
    "print(\"Dropped scaling projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90870748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create final projection with scaled properties for FastRP\n",
    "try:\n",
    "    gds.graph.drop('full_graph_fastrp_scaled')\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nStep 6: Creating final projection with scaled properties...\\n\")\n",
    "\n",
    "fastrp_projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source)-[r]->(target)\n",
    "    WITH gds.graph.project(\n",
    "        'full_graph_fastrp_scaled',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            sourceNodeProperties: source { \n",
    "                .scaledFeatures\n",
    "            },\n",
    "            targetNodeProperties: target { \n",
    "                .scaledFeatures\n",
    "            },\n",
    "            relationshipType: type(r)\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created projection for FastRP:\")\n",
    "display(fastrp_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run FastRP with scaled features\n",
    "print(\"\\nStep 7: Running FastRP with scaled degree and date features...\\n\")\n",
    "\n",
    "G = gds.graph.get('full_graph_fastrp_scaled')\n",
    "\n",
    "fastrp_result = gds.fastRP.write(\n",
    "    G,\n",
    "    writeProperty='fastrp_embedding_scaled',\n",
    "    embeddingDimension=128,\n",
    "    featureProperties=['scaledFeatures'],\n",
    "    iterationWeights=[0.0, 1.0, 1.0],\n",
    "    randomSeed=42\n",
    ")\n",
    "\n",
    "print(f\"FastRP completed successfully!\")\n",
    "print(f\"Properties written: {fastrp_result['nodePropertiesWritten']:,}\")\n",
    "\n",
    "pd.DataFrame([fastrp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.drop('full_graph_fastrp_scaled')\n",
    "print(\"Dropped existing projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create KNN projection with User nodes\n",
    "try:\n",
    "    gds.graph.drop('user_knn_scaled')\n",
    "    print(\"Dropped existing KNN projection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nStep 8: Creating projection for KNN with scaled embeddings...\\n\")\n",
    "\n",
    "knn_projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source:User)\n",
    "    WHERE source.fastrp_embedding_scaled IS NOT NULL\n",
    "    WITH gds.graph.project(\n",
    "        'user_knn_scaled',\n",
    "        source,\n",
    "        null,\n",
    "        {\n",
    "            sourceNodeProperties: source {\n",
    "                .fastrp_embedding_scaled\n",
    "            },\n",
    "            targetNodeProperties: {}\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created KNN projection:\")\n",
    "display(knn_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4069ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN to find similar users based on full graph embeddings\n",
    "print(\"Running KNN to find similar users based on full graph context...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "knn_results = gds.run_cypher(\"\"\"\n",
    "    CALL gds.knn.stream('user_knn_full_graph', {\n",
    "        nodeProperties: ['fastrp_embedding_scaled'],\n",
    "        topK: 5,\n",
    "        sampleRate: 1.0,\n",
    "        deltaThreshold: 0.0,\n",
    "        concurrency: 4,\n",
    "        similarityCutoff: 0.999\n",
    "    })\n",
    "    YIELD node1, node2, similarity\n",
    "    WITH gds.util.asNode(node1) AS u1, gds.util.asNode(node2) AS u2, similarity\n",
    "    WHERE u1.nameNormStrip IS NOT NULL \n",
    "      AND u2.nameNormStrip IS NOT NULL\n",
    "    RETURN u1.nameNormStrip AS name1,\n",
    "           u2.nameNormStrip AS name2,\n",
    "           u1.nameNormalized AS norm1,\n",
    "           u2.nameNormalized AS norm2,\n",
    "           similarity AS similarity_score\n",
    "    ORDER BY similarity DESC\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Found {len(knn_results)} similar pairs\")\n",
    "\n",
    "if len(knn_results) > 0:\n",
    "    print(f\"\\nKNN Similarity statistics:\")\n",
    "    print(f\"  Min: {knn_results['similarity_score'].min():.3f}\")\n",
    "    print(f\"  Max: {knn_results['similarity_score'].max():.3f}\")\n",
    "    print(f\"  Mean: {knn_results['similarity_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {knn_results['similarity_score'].median():.3f}\")\n",
    "    \n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    display(knn_results.head(100))\n",
    "else:\n",
    "    print(\"No similar pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.drop('user_knn_scaled')\n",
    "print(\"Dropped existing projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c732d",
   "metadata": {},
   "source": [
    "## 7. Resolving entities\n",
    "\n",
    "So, here we find ourselves with a few different methods for getting to high-quality resolutions.\n",
    "\n",
    "1. Community Detection (WCC, Leiden, Louvain) can help us to identify certain sectors within the graph.\n",
    "2. Node Similarity can help us to identify structurally similar nodes within the entire graph, or even down to just those sectors (good for when the graph is large)\n",
    "3. FastRP can help us to topologically encode every node in the graph + KNN allows us to get similarity another way (more approximate that Node Similarity)\n",
    "4. The basic text-matching rules which you may or may not be used to come into play at the very end of this process.\n",
    "\n",
    "The theory here is that, you can define some 'business rules' using a series of these filtration steps whereby 'if two people are this similar, and also have closely matching names, they are likely to be the same'. If you have enough supporting data, you could theoretically skip the name matching portion of this entirely.\n",
    "\n",
    "So, now, let's use everything we've done so far to identify a query that will allow us to resolve entities -- even with these limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8617b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the list of Louvain communities with their sizes\n",
    "# We'll process each community separately to keep memory manageable\n",
    "# Not wildly important at this scale, but good practice for larger graphs\n",
    "\n",
    "print(\"Getting Louvain communities to process...\\n\")\n",
    "\n",
    "communities = gds.run_cypher(\"\"\"\n",
    "    MATCH (u:User)\n",
    "    WHERE u.louvain_community IS NOT NULL\n",
    "      AND u.nameNormStrip IS NOT NULL\n",
    "      AND u.nameNormStrip <> ''\n",
    "    WITH u.louvain_community AS community, count(*) AS size\n",
    "    WHERE size >= 2  // Need at least 2 users to find matches\n",
    "    RETURN community, size\n",
    "    ORDER BY size DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Found {len(communities)} communities with 2+ users\")\n",
    "print(f\"Total users across communities: {communities['size'].sum():,}\")\n",
    "print(f\"\\nLargest 10 communities:\")\n",
    "display(communities.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def01c5e",
   "metadata": {},
   "source": [
    "This is essentially the resolution function we're defining. This cell just defines it; the next cell runs it in whatever way we specify.\n",
    "\n",
    "You could, for example, run this one by one per community or for every community, one after the other in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f28a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def resolve_entities_in_community(gds, community_id, similarity_cutoff=0.5, jw_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Run entity resolution for a single Louvain community.\n",
    "    \n",
    "    Steps:\n",
    "    1. Project User->Mailbox bipartite graph for this community\n",
    "    2. Run Node Similarity to find structurally similar users\n",
    "    3. Filter by Jaro-Winkler name similarity\n",
    "    \n",
    "    Parameters:\n",
    "    - community_id: The Louvain community ID to process\n",
    "    - similarity_cutoff: Minimum node similarity score (default 0.5)\n",
    "    - jw_threshold: Maximum Jaro-Winkler distance (default 0.15, i.e. 85% similarity)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame of matched entity pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    graph_name = f'community_{community_id}_bipartite'\n",
    "    \n",
    "    # Step 1: Clean up any existing projection\n",
    "    try:\n",
    "        gds.graph.drop(graph_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Step 2: Project Userâ†’Mailbox bipartite graph for this community\n",
    "    projection = gds.run_cypher(\"\"\"\n",
    "        MATCH (source:User)-[r:USED]->(target:Mailbox)\n",
    "        WHERE source.louvain_community = $community_id\n",
    "          AND target.email_degree IS NOT NULL\n",
    "          AND target.email_degree <= 19\n",
    "        WITH gds.graph.project(\n",
    "            $graph_name,\n",
    "            source,\n",
    "            target,\n",
    "            {\n",
    "                sourceNodeLabels: labels(source),\n",
    "                targetNodeLabels: labels(target),\n",
    "                relationshipType: type(r)\n",
    "            }\n",
    "        ) AS g\n",
    "        RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "    \"\"\", params={'community_id': community_id, 'graph_name': graph_name})\n",
    "    \n",
    "    # Step 3: Run Node Similarity\n",
    "    # This finds users who share similar mailbox connections\n",
    "    similarity_results = gds.run_cypher(\"\"\"\n",
    "        CALL gds.nodeSimilarity.stream($graph_name, {\n",
    "            similarityCutoff: $similarity_cutoff,\n",
    "            topK: 10,\n",
    "            similarityMetric: 'OVERLAP'\n",
    "        })\n",
    "        YIELD node1, node2, similarity\n",
    "        WITH gds.util.asNode(node1) AS u1, gds.util.asNode(node2) AS u2, similarity\n",
    "        WHERE u1:User AND u2:User\n",
    "          AND u1.nameNormStrip IS NOT NULL \n",
    "          AND u2.nameNormStrip IS NOT NULL\n",
    "          AND u1 < u2\n",
    "        \n",
    "        // Step 4: Apply Jaro-Winkler filtering\n",
    "        WITH u1, u2, similarity,\n",
    "             apoc.text.jaroWinklerDistance(\n",
    "                 toLower(u1.nameNormStrip), \n",
    "                 toLower(u2.nameNormStrip)\n",
    "             ) AS jw_distance\n",
    "        \n",
    "        WHERE jw_distance <= $jw_threshold\n",
    "        \n",
    "        RETURN \n",
    "            $community_id AS community,\n",
    "            u1.nameRaw AS user1_raw,\n",
    "            u2.nameRaw AS user2_raw,\n",
    "            u1.nameNormStrip AS user1_normalized,\n",
    "            u2.nameNormStrip AS user2_normalized,\n",
    "            round(similarity * 1000) / 1000.0 AS node_similarity,\n",
    "            round((1.0 - jw_distance) * 1000) / 1000.0 AS name_similarity,\n",
    "            round(jw_distance * 1000) / 1000.0 AS jw_distance\n",
    "        ORDER BY node_similarity ASC, name_similarity ASC\n",
    "    \"\"\", params={\n",
    "        'graph_name': graph_name, \n",
    "        'similarity_cutoff': similarity_cutoff,\n",
    "        'jw_threshold': jw_threshold,\n",
    "        'community_id': community_id\n",
    "    })\n",
    "    \n",
    "    # Step 5: Clean up projection\n",
    "    try:\n",
    "        gds.graph.drop(graph_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return similarity_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e07237",
   "metadata": {},
   "source": [
    "This cell is essentially just grabbing the communities we've already identified, and then running our resolution pass on each one in turn until it's complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENTITY RESOLUTION: Louvain â†’ Node Similarity â†’ Jaro-Winkler\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_CUTOFF = 0.5    # Minimum node similarity (shared mailbox overlap)\n",
    "JW_THRESHOLD = 0.15        # Maximum Jaro-Winkler distance (85% name similarity)\n",
    "\n",
    "all_matches = []\n",
    "communities_processed = 0\n",
    "communities_with_matches = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in communities.iterrows():\n",
    "    community_id = row['community']\n",
    "    community_size = row['size']\n",
    "    \n",
    "    # Progress indicator for large runs\n",
    "    if communities_processed % 50 == 0:\n",
    "        print(f\"Processing community {communities_processed + 1}/{len(communities)}...\")\n",
    "    \n",
    "    # Run resolution for this community\n",
    "    matches = resolve_entities_in_community(\n",
    "        gds, \n",
    "        community_id,\n",
    "        similarity_cutoff=SIMILARITY_CUTOFF,\n",
    "        jw_threshold=JW_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        all_matches.append(matches)\n",
    "        communities_with_matches += 1\n",
    "    \n",
    "    communities_processed += 1\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Combine all results\n",
    "if all_matches:\n",
    "    entity_matches = pd.concat(all_matches, ignore_index=True)\n",
    "else:\n",
    "    entity_matches = pd.DataFrame()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"  Communities processed: {communities_processed}\")\n",
    "print(f\"  Communities with matches: {communities_with_matches}\")\n",
    "print(f\"  Total entity matches found: {len(entity_matches)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d08906",
   "metadata": {},
   "source": [
    "Now that we've run it, let's take a look at the results. \n",
    "\n",
    "You'll notice here that I'm setting it to display in ascending order of confidence. This is so we can be certain that our worst performing match is still a valid match.\n",
    "\n",
    "In total, across the set of 58k User nodes -- on this run, I have found 4935 high-confidence matches, just by running a few algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show full content\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.width', None)         # Don't wrap to fit console width\n",
    "pd.set_option('display.max_seq_items', None) # Show all items in lists\n",
    "\n",
    "if len(entity_matches) > 0:\n",
    "    # Add confidence score based on both signals\n",
    "    # Higher node_similarity + higher name_similarity = higher confidence\n",
    "    entity_matches['confidence_score'] = (\n",
    "        0.5 * entity_matches['node_similarity'] + \n",
    "        0.5 * entity_matches['name_similarity']\n",
    "    ).round(3)\n",
    "    \n",
    "    # Assign confidence levels\n",
    "    entity_matches['match_confidence'] = pd.cut(\n",
    "        entity_matches['confidence_score'],\n",
    "        bins=[0, 0.70, 0.85, 1.0],\n",
    "        labels=['LOW', 'MEDIUM', 'HIGH']\n",
    "    )\n",
    "    \n",
    "    # Sort by confidence\n",
    "    entity_matches = entity_matches.sort_values('confidence_score', ascending=True)\n",
    "    \n",
    "    print(f\"\\nMatch Statistics:\")\n",
    "    print(f\"  HIGH confidence:   {(entity_matches['match_confidence'] == 'HIGH').sum()}\")\n",
    "    print(f\"  MEDIUM confidence: {(entity_matches['match_confidence'] == 'MEDIUM').sum()}\")\n",
    "    print(f\"  LOW confidence:    {(entity_matches['match_confidence'] == 'LOW').sum()}\")\n",
    "    print(f\"\\n  Node Similarity - Min: {entity_matches['node_similarity'].min():.3f}, \"\n",
    "          f\"Max: {entity_matches['node_similarity'].max():.3f}, \"\n",
    "          f\"Mean: {entity_matches['node_similarity'].mean():.3f}\")\n",
    "    print(f\"  Name Similarity - Min: {entity_matches['name_similarity'].min():.3f}, \"\n",
    "          f\"Max: {entity_matches['name_similarity'].max():.3f}, \"\n",
    "          f\"Mean: {entity_matches['name_similarity'].mean():.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LIKELY ENTITY MATCHES\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    display(entity_matches[[\n",
    "        'community',\n",
    "        'user1_raw', \n",
    "        'user2_raw',\n",
    "        'node_similarity',\n",
    "        'name_similarity',\n",
    "        'confidence_score',\n",
    "        'match_confidence'\n",
    "    ]])\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo matches found with current thresholds.\")\n",
    "    print(\"Consider adjusting SIMILARITY_CUTOFF or JW_THRESHOLD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8f634",
   "metadata": {},
   "source": [
    "## 8. Committing to the resolution\n",
    "\n",
    "Now, I'm happy with the results acheived in this run. You may want to experiment with higher or lower thresholds -- that's fine. \n",
    "\n",
    "My advice to you is to run this once, and commit to high-quality matches. Then, with your newly connected nodes, run it again -- the graph should tighten up over multiple runs and produce better and better matches until you hit diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f4cea",
   "metadata": {},
   "source": [
    "If you're ready to commit, you've got a few options:\n",
    "\n",
    "1. Create a new node to represent all of your SAME_AS nodes. You can then connect those nodes to it. (recommended)\n",
    "    - Pros: You get full traceability -- if something gets funky later, you can always undo\n",
    "    - Cons: Adds another layer of complexity to subsequent runs\n",
    "\n",
    "2. Merge all known SAME_AS to a single node, and have it inherit rels and properties. If you could be 100% sure that all of your resolutions are correct, you could do this.\n",
    "    - Pros: Easy to understand, simplifies subsequent runs, truly recursive\n",
    "    - Cons: No way to fix a bad merge, no traceability, fundamentally destructive\n",
    "\n",
    "I would advise adding a central node to represent your SAME_AS entities and connecting them to that. In later projections, it's easy enough to just include that entity as a source instead of its sub entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f38ca5",
   "metadata": {},
   "source": [
    "For each of our matches, we'll add a new relationship, 'SIM_RES'. Then we'll run WCC on those to identify a new group. This new group will represent our matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create SIMILAR_ENT relationships with all metrics\n",
    "# 2. Project graph with only SIMILAR_ENT relationships\n",
    "# 3. Run WCC to find connected components\n",
    "# 4. Create ResolvedEntity nodes per component\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING RESOLVED ENTITY NODES (WCC-BASED)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Create SIMILAR_ENT relationships between matched users with all metrics\n",
    "print(\"Step 1: Creating SIMILAR_ENT relationships between matched users...\\n\")\n",
    "\n",
    "similar_ent_result = gds.run_cypher(\"\"\"\n",
    "    UNWIND $matches AS match\n",
    "    MATCH (u1:User {nameNormStrip: match.user1_normalized})\n",
    "    MATCH (u2:User {nameNormStrip: match.user2_normalized})\n",
    "    WHERE u1.louvain_community = match.community\n",
    "      AND u2.louvain_community = match.community\n",
    "      AND u1 <> u2\n",
    "    \n",
    "    CALL (u1, u2, match) {\n",
    "        MERGE (u1)-[r:SIMILAR_ENT]-(u2)\n",
    "        ON CREATE SET \n",
    "            r.confidence_score = match.confidence_score,\n",
    "            r.node_similarity = match.node_similarity,\n",
    "            r.name_similarity = match.name_similarity,\n",
    "            r.jw_distance = match.jw_distance,\n",
    "            r.match_confidence = match.match_confidence,\n",
    "            r.created_at = datetime()\n",
    "    } IN TRANSACTIONS OF 100 ROWS\n",
    "    \n",
    "    RETURN count(*) AS relationships_created\n",
    "\"\"\", params={'matches': entity_matches.to_dict('records')})\n",
    "\n",
    "print(f\"  Created {similar_ent_result['relationships_created'].iloc[0]} SIMILAR_ENT relationships\")\n",
    "\n",
    "# Step 2: Project graph with User nodes and SIMILAR_ENT relationships\n",
    "print(\"\\nStep 2: Projecting graph for WCC analysis...\\n\")\n",
    "\n",
    "# Clean up any existing projection\n",
    "try:\n",
    "    gds.graph.drop('similar_entities_graph')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "projection = gds.run_cypher(\"\"\"\n",
    "    MATCH (source:User)-[r:SIMILAR_ENT]-(target:User)\n",
    "    WITH gds.graph.project(\n",
    "        'similar_entities_graph',\n",
    "        source,\n",
    "        target,\n",
    "        {\n",
    "            sourceNodeLabels: labels(source),\n",
    "            targetNodeLabels: labels(target),\n",
    "            relationshipType: type(r)\n",
    "        },\n",
    "        {\n",
    "            undirectedRelationshipTypes: ['*']\n",
    "        }\n",
    "    ) AS g\n",
    "    RETURN g.graphName AS name, g.nodeCount AS nodes, g.relationshipCount AS relationships\n",
    "\"\"\")\n",
    "\n",
    "print(f\"  Projected graph: {projection['nodes'].iloc[0]} nodes, {projection['relationships'].iloc[0]} relationships\")\n",
    "\n",
    "# Step 3: Run WCC on the projected graph\n",
    "print(\"\\nStep 3: Running WCC to find connected components...\\n\")\n",
    "\n",
    "G = gds.graph.get('similar_entities_graph')\n",
    "\n",
    "wcc_result = gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty='entity_resolution_wcc',\n",
    "    minComponentSize=1\n",
    ")\n",
    "\n",
    "print(f\"  WCC completed successfully!\")\n",
    "print(f\"  Components found: {wcc_result['componentCount']:,}\")\n",
    "print(f\"  Properties written: {wcc_result['nodePropertiesWritten']:,}\")\n",
    "\n",
    "# Clean up the projection\n",
    "gds.graph.drop('similar_entities_graph')\n",
    "\n",
    "# Step 4: Create ResolvedEntity nodes per WCC component\n",
    "print(\"\\nStep 4: Creating ResolvedEntity nodes per component...\\n\")\n",
    "\n",
    "resolved_result = gds.run_cypher(\"\"\"\n",
    "    // Get all users with entity_resolution_wcc property\n",
    "    MATCH (u:User)\n",
    "    WHERE u.entity_resolution_wcc IS NOT NULL\n",
    "    \n",
    "    // Group by component ID\n",
    "    WITH u.entity_resolution_wcc AS component_id, collect(u) AS members\n",
    "    WHERE size(members) >= 2\n",
    "    \n",
    "    // Create ResolvedEntity for each component\n",
    "    CREATE (re:ResolvedEntity {\n",
    "        wcc_id: component_id,\n",
    "        member_count: size(members),\n",
    "        resolution_method: 'louvain_nodesim_jarowinkler_wcc',\n",
    "        created_at: datetime()\n",
    "    })\n",
    "    \n",
    "    // Determine canonical name (most common normalized name, or longest)\n",
    "    WITH re, members\n",
    "    UNWIND members AS member\n",
    "    WITH re, members, member.nameNormStrip AS name, count(*) AS freq\n",
    "    ORDER BY freq DESC, size(name) DESC\n",
    "    WITH re, members, collect(name)[0] AS canonical_name\n",
    "    SET re.canonical_name = canonical_name\n",
    "    \n",
    "    // Collect all email addresses owned by members\n",
    "    WITH re, members\n",
    "    UNWIND members AS member\n",
    "    OPTIONAL MATCH (member)-[:USED]->(m:Mailbox)\n",
    "    WITH re, members, collect(DISTINCT m.address) AS emails\n",
    "    SET re.email_addresses = emails\n",
    "    \n",
    "    // Connect all users to the ResolvedEntity\n",
    "    WITH re, members\n",
    "    UNWIND members AS member\n",
    "    MERGE (member)-[r:RESOLVES_TO]->(re)\n",
    "    ON CREATE SET r.created_at = datetime()\n",
    "    \n",
    "    RETURN count(DISTINCT re) AS entities_created\n",
    "\"\"\")\n",
    "\n",
    "print(f\"  Created {resolved_result['entities_created'].iloc[0]} ResolvedEntity nodes\")\n",
    "\n",
    "# Step 5: Summary of created entities\n",
    "print(\"\\nStep 5: Reviewing created ResolvedEntity nodes...\\n\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "resolved_entities = gds.run_cypher(\"\"\"\n",
    "    MATCH (re:ResolvedEntity)<-[:RESOLVES_TO]-(u:User)\n",
    "    WITH re, collect(u.nameRaw) AS member_names, collect(DISTINCT u.nameNormStrip) AS normalized_names\n",
    "    RETURN \n",
    "        re.wcc_id AS wcc_id,\n",
    "        re.canonical_name AS canonical_name,\n",
    "        re.member_count AS member_count,\n",
    "        re.email_addresses[0..3] AS sample_emails,\n",
    "        member_names AS all_name_variants,\n",
    "        normalized_names AS unique_normalized_names\n",
    "    ORDER BY member_count DESC\n",
    "\"\"\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"RESOLUTION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"  ResolvedEntity nodes created: {len(resolved_entities)}\")\n",
    "print(f\"  Total users resolved: {resolved_entities['member_count'].sum()}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "display(resolved_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799beb65",
   "metadata": {},
   "source": [
    "\n",
    "Instead of projecting User nodes directly, you can now project ResolvedEntity:\n",
    "\n",
    "```\n",
    "MATCH (re:ResolvedEntity)-[:RESOLVES_TO]-(u:User)-[:SENT]->(e:Email)\n",
    "WITH re, e\n",
    "```\n",
    "\n",
    "... continue with your analysis using re instead of u\n",
    "\n",
    "Or to get all emails for a resolved entity:\n",
    "\n",
    "```\n",
    "MATCH (re:ResolvedEntity {canonical_name: 'kenneth lay'})<-[:RESOLVES_TO]-(u:User)\n",
    "MATCH (u)-[:SENT|RECEIVED|CC_ON|BCC_ON]-(e:Email)\n",
    "RETURN DISTINCT e\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c73e4e",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we successfully resolved duplicate entities using a multi-stage pipeline:\n",
    "\n",
    "1. **Community Detection** - Used Louvain to partition the graph into ~260 communities\n",
    "2. **Degree Centrality** - Filtered out generic/spam mailboxes (degree > 19)\n",
    "3. **Node Similarity** - Found users with overlapping mailbox connections\n",
    "4. **String Matching** - Validated matches with Jaro-Winkler (85%+ similarity)\n",
    "5. **Connected Components** - Grouped all aliases into ResolvedEntity nodes\n",
    "\n",
    "**Results**: ~5,000 high-confidence entity matches identified and linked\n",
    "\n",
    "### Using Resolved Entities\n",
    "\n",
    "Now that entities are resolved, you can query using `ResolvedEntity` nodes:\n",
    "\n",
    "```cypher\n",
    "// Find all emails for \"Kenneth Lay\" (all aliases combined)\n",
    "MATCH (re:ResolvedEntity {canonical_name: 'Kenneth Lay'})<-[:RESOLVES_TO]-(u:User)\n",
    "MATCH (u)-[:SENT|RECEIVED|CC_ON|BCC_ON]-(e:Email)\n",
    "RETURN DISTINCT e\n",
    "\n",
    "// Count emails per resolved entity\n",
    "MATCH (re:ResolvedEntity)<-[:RESOLVES_TO]-(u:User)\n",
    "MATCH (u)-[:SENT]->(e:Email)\n",
    "RETURN re.canonical_name, count(DISTINCT e) as emails_sent\n",
    "ORDER BY emails_sent DESC\n",
    "LIMIT 20\n",
    "\n",
    "// Find communication between resolved entities\n",
    "MATCH (re1:ResolvedEntity)<-[:RESOLVES_TO]-(u1:User)\n",
    "MATCH (re2:ResolvedEntity)<-[:RESOLVES_TO]-(u2:User)\n",
    "MATCH (u1)-[:SENT]->(e:Email)<-[:RECEIVED]-(u2)\n",
    "WHERE re1 <> re2\n",
    "RETURN re1.canonical_name, re2.canonical_name, count(e) as emails\n",
    "ORDER BY emails DESC\n",
    "LIMIT 10\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
