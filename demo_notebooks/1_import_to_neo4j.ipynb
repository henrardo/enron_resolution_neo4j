{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da5ed280",
      "metadata": {},
      "source": [
        "# Enron Email Dataset -> Neo4j Direct Import\n",
        "\n",
        "This notebook loads the raw Enron email dataset **directly into Neo4j** as a property graph.\n",
        "\n",
        "## Approach\n",
        "1. Parse raw email files\n",
        "2. Connect to Neo4j\n",
        "3. Create graph schema (constraints/indexes)\n",
        "4. Load emails as nodes and relationships\n",
        "5. Validate with Cypher queries\n",
        "\n",
        "## Graph Model\n",
        "```\n",
        "(User {nameRaw, primaryEmail, nameNormalized, associatedEmails})\n",
        "(Mailbox {address})\n",
        "(Email {date, folder, message_id, subject, thread, user, x_folder})\n",
        "\n",
        "(User)-[:SENT|RECEIVED|CC_ON|BCC_ON]->(Email)\n",
        "(User)-[:USED]->(Mailbox)\n",
        "```\n",
        "\n",
        "Note: The original files were re-redacted in 2006, so, while this is an interesting dataset, we can assume most of the super incriminating stuff is gone. Still, there are a good few examples throughout this dataset that hint at Enron Execs' activities. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a2b0cd",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "\n",
        "1. **Downloaded the Enron dataset** from: https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz\n",
        "2. **Extracted it** to the parent directory: `../maildir/`\n",
        "3. **Created a Neo4j database** named `neo4j`\n",
        "4. **Installed the GDS plugin** in Neo4j\n",
        "5. **Configured your `.env` file** with Neo4j credentials\n",
        "\n",
        "**Expected Runtime**: 60-90 minutes for full dataset (~517,000 emails)\n",
        "\n",
        "**Quick Test Option**: Set `LIMIT = 10000` in Cell 13 to process only 10,000 emails (~5 minutes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02bbc4f",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e6d38535",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphdatascience<2.0,>=1.8 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (1.18)\n",
            "Requirement already satisfied: neo4j<6.0,>=5.14 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (5.28.3)\n",
            "Requirement already satisfied: pandas<3.0,>=2.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (2.3.3)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.65 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: jellyfish<2.0,>=1.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: spacy<4.0,>=3.7 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 11)) (3.8.11)\n",
            "Requirement already satisfied: python-dotenv<2.0,>=1.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 14)) (1.2.1)\n",
            "Requirement already satisfied: anthropic<1.0,>=0.18 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 17)) (0.75.0)\n",
            "Requirement already satisfied: multimethod<3.0,>=1.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: numpy<2.4 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.3.5)\n",
            "Requirement already satisfied: pyarrow<23.0,>=17.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (22.0.0)\n",
            "Requirement already satisfied: textdistance<5.0,>=4.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: requests in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: tenacity>=9.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (9.1.2)\n",
            "Requirement already satisfied: pydantic>=2.11 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.12.5)\n",
            "Requirement already satisfied: pytz in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from neo4j<6.0,>=5.14->-r ../requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pandas<3.0,>=2.0->-r ../requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pandas<3.0,>=2.0->-r ../requirements.txt (line 6)) (2025.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (0.21.1)\n",
            "Requirement already satisfied: jinja2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (3.11)\n",
            "Requirement already satisfied: certifi in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic<1.0,>=0.18->-r ../requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic>=2.11->graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic>=2.11->graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from pydantic>=2.11->graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from requests->graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from requests->graphdatascience<2.0,>=1.8->-r ../requirements.txt (line 2)) (2.6.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.0->-r ../requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/.venv/lib/python3.13/site-packages (from jinja2->spacy<4.0,>=3.7->-r ../requirements.txt (line 11)) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32249d9",
      "metadata": {},
      "source": [
        "Start a new local database and install both apoc and GDS.\n",
        "\n",
        "Set up your .env file in line with the example env."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4533b592",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports loaded\n",
            "Environment variables loaded from .env\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from email import policy\n",
        "from email.parser import Parser\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "from neo4j import GraphDatabase\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Imports loaded\")\n",
        "print(\"Environment variables loaded from .env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e37ac8",
      "metadata": {},
      "source": [
        "## 2. Neo4j Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19a52c95",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to: neo4j://127.0.0.1:7687\n",
            "Database: enrongit\n",
            "User: neo4j\n",
            "  Neo4j Kernel: 2025.10.1\n",
            "  Cypher: 5\n",
            "Connected to Neo4j successfully\n"
          ]
        }
      ],
      "source": [
        "# Load Neo4j connection from environment variables\n",
        "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
        "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
        "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
        "DATABASE = \"enrongit\"\n",
        "\n",
        "if not NEO4J_PASSWORD:\n",
        "    raise ValueError(\"NEO4J_PASSWORD not found in .env file! Please copy .env.example to .env and configure it.\")\n",
        "\n",
        "print(f\"Connecting to: {NEO4J_URI}\")\n",
        "print(f\"Database: {DATABASE}\")\n",
        "print(f\"User: {NEO4J_USER}\")\n",
        "\n",
        "try:\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD), database=DATABASE)\n",
        "    driver.verify_connectivity()\n",
        "    \n",
        "    # Get version info\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions[0] as version\")\n",
        "        for record in result:\n",
        "            print(f\"  {record['name']}: {record['version']}\")\n",
        "    \n",
        "    print(\"Connected to Neo4j successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"  1. Is Neo4j running? Check Neo4j Desktop or run: systemctl status neo4j\")\n",
        "    print(\"  2. Does the 'neo4j' database exist?\")\n",
        "    print(\"  3. Are your credentials in .env correct?\")\n",
        "    print(\"  4. Is the URI correct? (neo4j:// for bolt, bolt:// also works)\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b881cad",
      "metadata": {},
      "source": [
        "## 3. Create Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "83f6414e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema created\n"
          ]
        }
      ],
      "source": [
        "with driver.session() as session:\n",
        "    # Constraints - composite keys for User\n",
        "    session.run(\"CREATE CONSTRAINT user_email_key IF NOT EXISTS FOR (u:User) REQUIRE (u.nameRaw, u.primaryEmail) IS UNIQUE\")\n",
        "    session.run(\"CREATE CONSTRAINT user_imceanotes_key IF NOT EXISTS FOR (u:User) REQUIRE (u.nameRaw, u.primaryImceanotes) IS UNIQUE\")\n",
        "    session.run(\"CREATE CONSTRAINT email_id IF NOT EXISTS FOR (e:Email) REQUIRE e.message_id IS UNIQUE\")\n",
        "    session.run(\"CREATE CONSTRAINT mailbox_address IF NOT EXISTS FOR (m:Mailbox) REQUIRE m.address IS UNIQUE\")\n",
        "    \n",
        "    # Indexes\n",
        "    session.run(\"CREATE INDEX email_date IF NOT EXISTS FOR (e:Email) ON (e.date)\")\n",
        "    session.run(\"CREATE INDEX email_subject IF NOT EXISTS FOR (e:Email) ON (e.subject)\")\n",
        "    session.run(\"CREATE INDEX user_name_raw IF NOT EXISTS FOR (u:User) ON (u.nameRaw)\")\n",
        "    session.run(\"CREATE INDEX user_name_normalized IF NOT EXISTS FOR (u:User) ON (u.nameNormalized)\")\n",
        "    \n",
        "print(\"Schema created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab50ecdf",
      "metadata": {},
      "source": [
        "## 4. Email Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632d361d",
      "metadata": {},
      "source": [
        "This is just one way to parse these emails. \n",
        "\n",
        "Bear in mind, there are already email parsers out there, both generally, and which are specifically made to parse this dataset. However, we're not using those here, because your data may not look anything like that.\n",
        "\n",
        "Before you run it, it would be worth checking out the actual email files, so you can understand why certain decisions have been made.\n",
        "\n",
        "## Why is Email Parsing Complex?\n",
        "\n",
        "The Enron dataset contains messy, real-world email data with multiple inconsistent formats:\n",
        "\n",
        "### Challenge 1: Duplicate Headers\n",
        "Each email has **two sets of headers**:\n",
        "- **X-headers** (X-From, X-To, X-cc): Contain display names like \"Kenneth Lay\"\n",
        "- **Regular headers** (From, To, Cc): Contain email addresses like \"kenneth.lay@enron.com\"\n",
        "\n",
        "These don't always match perfectly. Sometimes \"Ken Lay\" in X-From corresponds to \"klay@enron.com\" in From.\n",
        "\n",
        "### Challenge 2: IMCEANOTES Identifiers\n",
        "Some participants appear as cryptic IMCEANOTES strings instead of proper emails:\n",
        "```\n",
        "IMCEANOTES-Michael+20Maggi+20+3CMichael+2EMaggi+40ENRON+2Ecom+3E@ENRON.com\n",
        "```\n",
        "These are Microsoft Exchange artifacts that need special handling.\n",
        "\n",
        "### Challenge 3: Inconsistent Name Formats\n",
        "Names appear in multiple formats:\n",
        "- \"Kenneth Lay\" vs \"Lay, Kenneth\" vs \"Kenneth L. Lay\"\n",
        "- \"Ken Lay (E-mail)\" vs \"Ken Lay\"\n",
        "- \"/O=ENRON/OU=NA/CN=RECIPIENTS/CN=KLAY\"\n",
        "\n",
        "### Challenge 4: Mismatched Pairs\n",
        "Sometimes the name in X-From doesn't correspond to the email in From:\n",
        "- X-From: \"John Smith\"\n",
        "- From: \"random.person@enron.com\"\n",
        "\n",
        "We need to validate these pairs before linking them.\n",
        "\n",
        "## Our Parsing Strategy\n",
        "\n",
        "The `ParticipantExtractor` class handles these challenges:\n",
        "\n",
        "1. **Extract all parts separately**: Name, email, and IMCEANOTES from each header\n",
        "2. **Combine strategically**: Prefer X-headers for names, regular headers for emails\n",
        "3. **Validate with Jaro-Winkler**: Use string similarity to check if name matches email\n",
        "   - \"kenneth.lay\" vs \"Kenneth Lay\" -> 95%+ similarity ✓ Link them\n",
        "   - \"john.smith\" vs \"Kenneth Lay\" -> Low similarity ✗ Keep separate\n",
        "4. **Create flexible nodes**:\n",
        "   - `User` nodes from names (may have multiple per person initially)\n",
        "   - `Mailbox` nodes from email addresses (unique)\n",
        "   - `User` <-> `Mailbox` via `USED` relationship\n",
        "\n",
        "This approach preserves **traceability** - we can see what data came from where and fix issues later in the entity resolution phase (Notebook 2).\n",
        "\n",
        "## The Graph Model\n",
        "\n",
        "```\n",
        "(User {nameRaw, nameNormalized, primaryEmail, associatedEmails[]})\n",
        "(Mailbox {address})\n",
        "(Email {message_id, date, subject, thread})\n",
        "\n",
        "Relationships:\n",
        "(User)-[:USED]->(Mailbox)\n",
        "(Mailbox)-[:SENT|RECEIVED|CC_ON|BCC_ON]->(Email)\n",
        "(User)-[:SENT|RECEIVED|CC_ON|BCC_ON]->(Email)\n",
        "```\n",
        "\n",
        "**Note**: We'll have many duplicate `User` nodes initially (e.g., \"Ken Lay\", \"Kenneth Lay\", \"Lay, Kenneth\"). That's intentional! Notebook 2 will resolve these into unified entities.\n",
        "\n",
        "Also, we could use something like GLiNER or spaCy for extracting entities. However, the threads are so variable, I think it's better to go with something deterministic to begin with. Later, we'll use tools like spaCy and GLiNER to extract individual messages from threads, and the entities mentioned within them followed by another resolution pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "be5ae267",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parser ready\n"
          ]
        }
      ],
      "source": [
        "# 4.1 Parser\n",
        "import re\n",
        "import quopri\n",
        "import jellyfish\n",
        "from email import policy\n",
        "from email.parser import Parser\n",
        "from email.utils import parsedate_to_datetime\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ParsedParts:\n",
        "    \"\"\"Container for extracted participant parts.\"\"\"\n",
        "    name_raw: Optional[str] = None\n",
        "    name_normalized: Optional[str] = None\n",
        "    email: Optional[str] = None\n",
        "    imceanotes: Optional[str] = None\n",
        "    \n",
        "    def to_participant(self) -> Optional[Dict]:\n",
        "        \"\"\"Convert to the participant dict format expected by the importer.\"\"\"\n",
        "        if not self.name_raw and not self.email:\n",
        "            return None\n",
        "        \n",
        "        person = None\n",
        "        if self.name_raw:\n",
        "            person = {'raw': self.name_raw, 'normalized': self.name_normalized}\n",
        "            if self.email:\n",
        "                person['associated_email'] = self.email\n",
        "            if self.imceanotes:\n",
        "                person['imceanotes_id'] = self.imceanotes\n",
        "        \n",
        "        return {\n",
        "            'person': person,\n",
        "            'mailbox': {'address': self.email} if self.email else None,\n",
        "            'linked': bool(self.name_raw and self.email)\n",
        "        }\n",
        "\n",
        "\n",
        "class ParticipantExtractor:\n",
        "    \"\"\"Extract User and Mailbox entities from email header fields.\"\"\"\n",
        "    \n",
        "    EMAIL_RE = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+(?:\\.[a-zA-Z]{2,})?')\n",
        "    NAME_EMAIL_RE = re.compile(r'^([^<]+?)\\s*<([^>]+)>$')\n",
        "    UNDISCLOSED_RE = re.compile(r'undisclosed[- ]?recipients?', re.I)\n",
        "    ENRON_PATH_RE = re.compile(r'/O=ENRON/.*?/CN=([A-Za-z0-9]+)', re.I)\n",
        "    IMCEANOTES_RE = re.compile(r'(IMCEANOTES-[^@]+@[^>\\s,]+)', re.I)\n",
        "    \n",
        "    def name_to_email_similarity(self, name: str, email: str) -> float:\n",
        "        \"\"\"Calculate best Jaro-Winkler similarity between name variants and email local part.\"\"\"\n",
        "        if not name or not email:\n",
        "            return 0.0\n",
        "        \n",
        "        local = email.split('@')[0].lower()\n",
        "        parts = name.lower().split()\n",
        "        \n",
        "        variants = [name.lower().replace(' ', '.')]\n",
        "        if len(parts) >= 2:\n",
        "            variants.extend([\n",
        "                f\"{parts[0]}.{parts[-1]}\",\n",
        "                f\"{parts[-1]}.{parts[0]}\",\n",
        "                f\"{parts[0][0]}.{parts[-1]}\"\n",
        "            ])\n",
        "        \n",
        "        return max(jellyfish.jaro_winkler_similarity(local, v) for v in variants)\n",
        "    \n",
        "    def clean_email(self, email: str) -> Optional[str]:\n",
        "        \"\"\"Clean and validate an email address.\"\"\"\n",
        "        if not email:\n",
        "            return None\n",
        "        \n",
        "        email = email.strip().lower()\n",
        "        \n",
        "        if email.startswith(('imceanotes-', '/')) or '@' not in email:\n",
        "            return None\n",
        "        \n",
        "        # Normalize @enron to @enron.com\n",
        "        if email.endswith('@enron'):\n",
        "            email += '.com'\n",
        "        \n",
        "        if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+$', email):\n",
        "            return None\n",
        "        \n",
        "        return email\n",
        "    \n",
        "    def clean_name(self, name: str) -> Optional[tuple[str, str]]:\n",
        "        \"\"\"Clean and validate a name. Returns (raw, normalized) or None.\"\"\"\n",
        "        if not name:\n",
        "            return None\n",
        "        \n",
        "        # Clean up artifacts\n",
        "        name = re.sub(r'@ENRON\\s*$|/[A-Z]=[^/]+|<[^>]+>', '', name, flags=re.I)\n",
        "        name = ' '.join(name.strip().strip('\"\\'').split())\n",
        "        \n",
        "        if len(name) < 3 or not re.search(r'[a-zA-Z]', name) or '@' in name or name.startswith('/'):\n",
        "            return None\n",
        "        \n",
        "        # Handle \"Last, First\" format\n",
        "        if ',' in name:\n",
        "            parts = [p.strip() for p in name.split(',', 1)]\n",
        "            if len(parts) == 2 and len(parts[0]) >= 2 and len(parts[1]) >= 2:\n",
        "                name = f\"{parts[1]} {parts[0]}\"\n",
        "        \n",
        "        # Require first + last name, each at least 2 chars\n",
        "        name_parts = name.split()\n",
        "        if len(name_parts) < 2 or len(name_parts[0]) < 2 or len(name_parts[-1]) < 2:\n",
        "            return None\n",
        "        \n",
        "        return (name, name.title())\n",
        "    \n",
        "    def extract(self, text: str) -> ParsedParts:\n",
        "        \"\"\"Extract all parts from a single participant string.\"\"\"\n",
        "        if not text:\n",
        "            return ParsedParts()\n",
        "        \n",
        "        text = re.sub(r'<\\?\\?S[^>]*>', '', text).strip()\n",
        "        if not text:\n",
        "            return ParsedParts()\n",
        "        \n",
        "        parts = ParsedParts()\n",
        "        \n",
        "        # Extract IMCEANOTES\n",
        "        if match := self.IMCEANOTES_RE.search(text):\n",
        "            parts.imceanotes = match.group(1).lower()\n",
        "        \n",
        "        # Extract first valid email (not IMCEANOTES)\n",
        "        for match in self.EMAIL_RE.finditer(text):\n",
        "            if cleaned := self.clean_email(match.group()):\n",
        "                parts.email = cleaned\n",
        "                break\n",
        "        \n",
        "        # Try Enron path if no email found\n",
        "        if not parts.email and (match := self.ENRON_PATH_RE.search(text)):\n",
        "            parts.email = f\"{match.group(1).lower()}@enron.com\"\n",
        "        \n",
        "        # Extract name\n",
        "        if match := self.NAME_EMAIL_RE.match(text):\n",
        "            name_result = self.clean_name(match.group(1))\n",
        "        else:\n",
        "            cleaned_text = re.sub(r'<[^>]*>|@ENRON\\s*$', '', text, flags=re.I).strip()\n",
        "            name_result = self.clean_name(cleaned_text) if '@' not in cleaned_text else None\n",
        "        \n",
        "        if name_result:\n",
        "            parts.name_raw, parts.name_normalized = name_result\n",
        "        \n",
        "        return parts\n",
        "    \n",
        "    def extract_list(self, header: str) -> List[ParsedParts]:\n",
        "        \"\"\"Extract all parts from a comma-separated header.\"\"\"\n",
        "        if not header:\n",
        "            return []\n",
        "        \n",
        "        if self.UNDISCLOSED_RE.search(header):\n",
        "            return [ParsedParts(email='undisclosed-recipients')]\n",
        "        \n",
        "        # Split on commas, respecting brackets\n",
        "        results = []\n",
        "        current, depth = \"\", 0\n",
        "        \n",
        "        for char in header:\n",
        "            if char in '<(':\n",
        "                depth += 1\n",
        "            elif char in '>)':\n",
        "                depth -= 1\n",
        "            elif char == ',' and depth == 0:\n",
        "                if current.strip():\n",
        "                    results.append(self.extract(current))\n",
        "                current = \"\"\n",
        "                continue\n",
        "            current += char\n",
        "        \n",
        "        if current.strip():\n",
        "            results.append(self.extract(current))\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "class EmailParser:\n",
        "    def __init__(self):\n",
        "        self.parser = Parser(policy=policy.default)\n",
        "        self.extractor = ParticipantExtractor()\n",
        "    \n",
        "    def parse(self, file_path) -> Optional[Dict]:\n",
        "        \"\"\"Parse an email file.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                raw_content = f.read()\n",
        "            \n",
        "            msg = self.parser.parsestr(raw_content)\n",
        "            parts = file_path.parts\n",
        "            \n",
        "            # Extract folder info\n",
        "            try:\n",
        "                idx = parts.index('maildir')\n",
        "                user = parts[idx + 1] if len(parts) > idx + 1 else None\n",
        "                folder = '/'.join(parts[idx + 2:-1]) if len(parts) > idx + 2 else None\n",
        "            except ValueError:\n",
        "                user = parts[-3] if len(parts) >= 3 else None\n",
        "                folder = parts[-2] if len(parts) >= 2 else None\n",
        "            \n",
        "            # Parse date\n",
        "            date = None\n",
        "            try:\n",
        "                if dt := parsedate_to_datetime(msg.get('Date', '')):\n",
        "                    date = dt.replace(tzinfo=None).strftime('%Y-%m-%dT%H:%M:%S')\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # Decode thread content\n",
        "            thread = raw_content[:50000]\n",
        "            if '=20' in thread or '=\\n' in thread:\n",
        "                try:\n",
        "                    thread = quopri.decodestring(thread.encode()).decode('utf-8', errors='ignore')\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            return {\n",
        "                'message_id': msg.get('Message-ID', '').strip('<>'),\n",
        "                'date': date,\n",
        "                'subject': msg.get('Subject', ''),\n",
        "                'thread': thread,\n",
        "                'from': self._combine_single(msg.get('X-From', ''), msg.get('From', '')),\n",
        "                'to': self._combine_list(msg.get('X-To', ''), msg.get('To', '')),\n",
        "                'cc': self._combine_list(msg.get('X-cc', ''), msg.get('Cc', '')),\n",
        "                'bcc': self._combine_list(msg.get('X-bcc', ''), msg.get('Bcc', '')),\n",
        "                'user': user,\n",
        "                'folder': folder,\n",
        "                'x_folder': msg.get('X-Folder', ''),\n",
        "            }\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    def _merge_parts(self, x: ParsedParts, reg: ParsedParts, validate: bool = True) -> List[Dict]:\n",
        "        \"\"\"Merge X-header and regular header parts, optionally validating with Jaro-Winkler.\"\"\"\n",
        "        name_raw = x.name_raw or reg.name_raw\n",
        "        name_norm = x.name_normalized or reg.name_normalized\n",
        "        email = reg.email or x.email  # Prefer regular header for email\n",
        "        imceanotes = x.imceanotes or reg.imceanotes\n",
        "        \n",
        "        # Validate match if both name and email present\n",
        "        if validate and name_raw and email:\n",
        "            if self.extractor.name_to_email_similarity(name_norm, email) < 0.95:\n",
        "                # Poor match - return separate entries\n",
        "                results = []\n",
        "                if p := x.to_participant():\n",
        "                    results.append(p)\n",
        "                if p := reg.to_participant():\n",
        "                    results.append(p)\n",
        "                return results\n",
        "        \n",
        "        # Good match or no validation needed\n",
        "        merged = ParsedParts(name_raw, name_norm, email, imceanotes)\n",
        "        if p := merged.to_participant():\n",
        "            return [p]\n",
        "        return []\n",
        "    \n",
        "    def _combine_single(self, x_header: str, reg_header: str) -> Optional[Dict]:\n",
        "        \"\"\"Combine X-header and regular header for a single participant.\"\"\"\n",
        "        x_parts = self.extractor.extract(x_header)\n",
        "        reg_parts = self.extractor.extract(reg_header)\n",
        "        results = self._merge_parts(x_parts, reg_parts, validate=False)\n",
        "        return results[0] if results else {'person': None, 'mailbox': None, 'linked': False}\n",
        "    \n",
        "    def _combine_list(self, x_header: str, reg_header: str) -> List[Dict]:\n",
        "        \"\"\"Combine X-header and regular header lists with positional matching.\"\"\"\n",
        "        x_list = self.extractor.extract_list(x_header)\n",
        "        reg_list = self.extractor.extract_list(reg_header)\n",
        "        \n",
        "        if not x_list and not reg_list:\n",
        "            return []\n",
        "        \n",
        "        # Single list case\n",
        "        if not x_list:\n",
        "            return [p for parts in reg_list if (p := parts.to_participant())]\n",
        "        if not reg_list:\n",
        "            return [p for parts in x_list if (p := parts.to_participant())]\n",
        "        \n",
        "        # Both lists - positional matching with validation\n",
        "        results = []\n",
        "        min_len = min(len(x_list), len(reg_list))\n",
        "        \n",
        "        for i in range(min_len):\n",
        "            results.extend(self._merge_parts(x_list[i], reg_list[i], validate=True))\n",
        "        \n",
        "        # Handle remainders\n",
        "        for parts in x_list[min_len:] + reg_list[min_len:]:\n",
        "            if p := parts.to_participant():\n",
        "                results.append(p)\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "parser = EmailParser()\n",
        "print(\"Parser ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2890c80",
      "metadata": {},
      "source": [
        "## 5. Collect Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "7daf2e96",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning dataset at: /Users/henryadamcollie/Documents/GitHub/enron_resolution_neo4j/demo_notebooks/../maildir\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scanning: 100%|██████████| 151/151 [00:15<00:00,  9.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10,000 files (limited to 10,000 for testing)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 5.1 Collect Files\n",
        "MAILDIR = Path(\"../maildir\")\n",
        "\n",
        "# CONFIGURATION: Set LIMIT for testing or None for full dataset\n",
        "LIMIT = 10000  # Options: None (full ~517k emails), 10000 (quick test ~5 min), 50000 (medium test ~20 min)\n",
        "\n",
        "# Validate dataset exists\n",
        "if not MAILDIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset not found at {MAILDIR.absolute()}\\n\\n\"\n",
        "        f\"Please download from: https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz\\n\"\n",
        "        f\"Extract to: {MAILDIR.parent.absolute()}/\\n\"\n",
        "        f\"Expected structure: {MAILDIR.absolute()}/[username]/[folder]/[emails]\"\n",
        "    )\n",
        "\n",
        "print(f\"Scanning dataset at: {MAILDIR.absolute()}\")\n",
        "\n",
        "files = []\n",
        "for user in tqdm(sorted(MAILDIR.iterdir()), desc=\"Scanning\"):\n",
        "    if user.is_dir():\n",
        "        files.extend([f for f in user.rglob(\"*\") if f.is_file() and not f.name.startswith('.')])\n",
        "\n",
        "if LIMIT:\n",
        "    files = files[:LIMIT]\n",
        "    print(f\"Found {len(files):,} files (limited to {LIMIT:,} for testing)\")\n",
        "else:\n",
        "    print(f\"Found {len(files):,} files (full dataset)\")\n",
        "    print(f\"Processing all files will take 60-90 minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2a4c8472",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed: 36.\n",
            "From: {'person': {'raw': 'Christi L Nicolay', 'normalized': 'Christi L Nicolay', 'associated_email': 'christi.nicolay@enron.com'}, 'mailbox': {'address': 'christi.nicolay@enron.com'}, 'linked': True}\n",
            "To: [{'person': {'raw': 'Phillip K Allen', 'normalized': 'Phillip K Allen', 'associated_email': 'phillip.allen@enron.com'}, 'mailbox': {'address': 'phillip.allen@enron.com'}, 'linked': True}]...\n",
            "Subject: Re: Talking points about California Gas market...\n"
          ]
        }
      ],
      "source": [
        "# Test parser on a single file\n",
        "test_file = files[0]\n",
        "sample = parser.parse(test_file)\n",
        "print(f\"Parsed: {test_file.name}\")\n",
        "print(f\"From: {sample['from']}\")\n",
        "print(f\"To: {sample['to'][:2]}...\")  # First 2 recipients\n",
        "print(f\"Subject: {sample['subject'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "80020db0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1: Message-ID: <12357410.1075855679611.JavaMail.evans@thyme>\n",
            "  2: Date: Tue, 12 Dec 2000 04:41:00 -0800 (PST)\n",
            "  3: From: christi.nicolay@enron.com\n",
            "  4: To: phillip.allen@enron.com\n",
            "  5: Subject: Re: Talking points about California Gas market\n",
            "  6: Mime-Version: 1.0\n",
            "  7: Content-Type: text/plain; charset=us-ascii\n",
            "  8: Content-Transfer-Encoding: 7bit\n",
            "  9: X-From: Christi L Nicolay\n",
            " 10: X-To: Phillip K Allen\n",
            " 11: X-cc:\n",
            " 12: X-bcc:\n",
            " 13: X-Folder: \\Phillip_Allen_Dec2000\\Notes Folders\\Notes inbox\n",
            " 14: X-Origin: Allen-P\n",
            " 15: X-FileName: pallen.nsf\n",
            " 16: \n",
            " 17: Phillip--To the extent that we can give Chair Hoecker our spin on the reasons\n",
            " 18: for the hikes, we would like to.  The Commission is getting calls from\n",
            " 19: legislators, DOE, etc. about the prices and is going to have to provide some\n",
            " 20: response.  Better if it coincides with Enron's view and is not anti-market.\n",
            " 21: We still haven't decided what we will provide.  You definitely will be\n",
            " 22: included in that discussion once we get the numbers from accounting.  Thanks.\n",
            " 23: \n",
            " 24: \n",
            " 25: \n",
            " 26: \n",
            " 27: \n",
            " 28: \tFrom:  Phillip K Allen                           12/12/2000 12:03 PM\n",
            " 29: \n",
            " 30: \n",
            " 31: To: Christi L Nicolay/HOU/ECT@ECT\n",
            " 32: cc:\n",
            " 33: \n",
            " 34: Subject: Talking points about California Gas market\n",
            " 35: \n",
            " 36: Christy,\n",
            " 37: \n",
            " 38:  I read these points and they definitely need some touch up.  I don't\n",
            " 39: understand why we need to give our commentary on  why prices are so high in\n",
            " 40: California.  This subject has already gotten so much press.\n",
            " 41: \n",
            " 42: Phillip\n",
            " 43: \n",
            " 44: \n",
            " 45: \n",
            " 46: \n",
            " 47: \n",
            " 48: \n",
            " 49: ---------------------- Forwarded by Phillip K Allen/HOU/ECT on 12/12/2000\n",
            " 50: 12:01 PM ---------------------------\n",
            " 51: From: Leslie Lawner@ENRON on 12/12/2000 11:56 AM CST\n",
            " 52: To: Christi L Nicolay/HOU/ECT@ECT, Joe Hartsoe/Corp/Enron@ENRON, Rebecca W\n",
            " 53: Cantrell/HOU/ECT@ECT, Ruth Concannon/HOU/ECT@ECT, Stephanie\n",
            " 54: Miller/Corp/Enron@ENRON, Phillip K Allen/HOU/ECT@ECT, Jane M\n",
            " 55: Tholt/HOU/ECT@ECT, Richard Shapiro/NA/Enron@Enron\n",
            " 56: cc:\n",
            " 57: Subject: Talking points about California Gas market\n",
            " 58: \n",
            " 59: Here is my stab at the talking points  to be sent in to FERC along with the\n",
            " 60: gas pricing info they requested for the California markets.  Let me or\n",
            " 61: Christi know if you have any disagreements, additions, whatever.  I am\n",
            " 62: supposed to be out of here at 2:15 today, so if you have stuff to add after\n",
            " 63: that, get it to Christi.  Thanks.\n",
            " 64: \n",
            " 65: \n",
            " 66: \n",
            " 67: \n",
            " 68: \n",
            "\n",
            "======================================================================\n",
            "PARSED OUTPUT\n",
            "======================================================================\n",
            "Message ID: 12357410.1075855679611.JavaMail.evans@thyme\n",
            "Date:       2000-12-12T04:41:00\n",
            "Folder:     allen-p/notes_inbox\n",
            "Subject:    Re: Talking points about California Gas market\n",
            "FROM:\n",
            "   Name:    Christi L Nicolay\n",
            "   Email:   christi.nicolay@enron.com\n",
            "   Mailbox: christi.nicolay@enron.com\n",
            "TO: (1 recipients)\n",
            "   1. Phillip K Allen <phillip.allen@enron.com>\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# TEST: Parse a single email to see the structure\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "test_file = files[0]\n",
        "\n",
        "# Show the raw email first\n",
        "with open(test_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    lines = f.readlines()[:80]\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        print(f\"{i:3}: {line.rstrip()}\")\n",
        "    if len(lines) == 80:\n",
        "        print(\"     ... (truncated)\")\n",
        "\n",
        "# Now show what the parser extracts\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARSED OUTPUT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample = parser.parse(test_file)\n",
        "\n",
        "print(f\"Message ID: {sample['message_id']}\")\n",
        "print(f\"Date:       {sample['date']}\")\n",
        "print(f\"Folder:     {sample['user']}/{sample['folder']}\")\n",
        "print(f\"Subject:    {sample['subject'][:60]}{'...' if len(sample['subject']) > 60 else ''}\")\n",
        "\n",
        "print(f\"FROM:\")\n",
        "frm = sample['from']\n",
        "if frm['person']:\n",
        "    print(f\"   Name:    {frm['person']['raw']}\")\n",
        "    print(f\"   Email:   {frm['person'].get('associated_email', 'N/A')}\")\n",
        "if frm['mailbox']:\n",
        "    print(f\"   Mailbox: {frm['mailbox']['address']}\")\n",
        "\n",
        "print(f\"TO: ({len(sample['to'])} recipients)\")\n",
        "for i, recip in enumerate(sample['to'][:3]):  # Show first 3\n",
        "    name = recip['person']['raw'] if recip['person'] else None\n",
        "    addr = recip['mailbox']['address'] if recip['mailbox'] else None\n",
        "    print(f\"   {i+1}. {name or '(no name)'} <{addr or '(no email)'}>\")\n",
        "if len(sample['to']) > 3:\n",
        "    print(f\"   ... and {len(sample['to']) - 3} more\")\n",
        "\n",
        "if sample['cc']:\n",
        "    print(f\"CC: ({len(sample['cc'])} recipients)\")\n",
        "    for i, recip in enumerate(sample['cc'][:2]):\n",
        "        name = recip['person']['raw'] if recip['person'] else None\n",
        "        addr = recip['mailbox']['address'] if recip['mailbox'] else None\n",
        "        print(f\"   {i+1}. {name or '(no name)'} <{addr or '(no email)'}>\")\n",
        "    if len(sample['cc']) > 2:\n",
        "        print(f\"   ... and {len(sample['cc']) - 2} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a9670b7",
      "metadata": {},
      "source": [
        "## 6. Import to Neo4j\n",
        "\n",
        "This cell processes all emails and loads them into Neo4j.\n",
        "\n",
        "**Runtime Warning**: \n",
        "- Full dataset (~517k emails): 60-90 minutes\n",
        "- 50k emails: ~20 minutes  \n",
        "- 10k emails: ~5 minutes\n",
        "\n",
        "The import creates:\n",
        "- Email nodes with message content\n",
        "- User nodes from display names\n",
        "- Mailbox nodes from email addresses\n",
        "- Relationships: SENT, RECEIVED, CC_ON, BCC_ON, USED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "77c23c94",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Importing: 100%|██████████| 10000/10000 [01:46<00:00, 93.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Imported 10,000 emails (0 errors)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 6.1 Import Participants\n",
        "def import_participant(tx, participant: Dict, message_id: str, rel_type: str):\n",
        "    \"\"\"Import a participant (User and/or Mailbox) and create relationships.\"\"\"\n",
        "    if not participant:\n",
        "        return\n",
        "    \n",
        "    user = participant.get('person')\n",
        "    mailbox = participant.get('mailbox')\n",
        "    linked = participant.get('linked', False)\n",
        "    parsing_error = participant.get('parsing_error', False)\n",
        "    \n",
        "    has_user = user is not None\n",
        "    has_mailbox = mailbox and mailbox.get('address')\n",
        "    incomplete = not (has_user and has_mailbox)\n",
        "    \n",
        "    # Create Mailbox and connect to Email\n",
        "    # Note on transactions:\n",
        "    # session.execute_write(func, args) calls func(tx, args) where tx is a transaction object.\n",
        "    # All tx.run() calls within the function either succeed together or roll back together.\n",
        "    # This ensures we don't end up with partial data if something fails mid-import.\n",
        "    if has_mailbox:\n",
        "        tx.run(f\"\"\"\n",
        "            MERGE (m:Mailbox {{address: $address}})\n",
        "            WITH m\n",
        "            MATCH (e:Email {{message_id: $message_id}})\n",
        "            MERGE (m)-[r:{rel_type}]->(e)\n",
        "            SET r.incompletePair = $incomplete\n",
        "        \"\"\", address=mailbox['address'], message_id=message_id, incomplete=incomplete)\n",
        "    \n",
        "    # Create User and connect to Email\n",
        "    if has_user:\n",
        "        associated_email = user.get('associated_email')\n",
        "        imceanotes_id = user.get('imceanotes_id')\n",
        "        user_parsing_error = user.get('parsing_error', False) or parsing_error\n",
        "        \n",
        "        if associated_email:\n",
        "            # MERGE on nameRaw + associated_email pair\n",
        "            tx.run(f\"\"\"\n",
        "                MERGE (u:User {{nameRaw: $nameRaw, primaryEmail: $email}})\n",
        "                SET u.nameNormalized = $nameNormalized\n",
        "                SET u.associatedEmails = CASE \n",
        "                    WHEN u.associatedEmails IS NULL THEN [$email]\n",
        "                    WHEN NOT $email IN u.associatedEmails THEN u.associatedEmails + $email\n",
        "                    ELSE u.associatedEmails\n",
        "                END\n",
        "                SET u.imceanotes = CASE \n",
        "                    WHEN $imceanotes_id IS NULL THEN u.imceanotes\n",
        "                    WHEN u.imceanotes IS NULL THEN [$imceanotes_id]\n",
        "                    WHEN NOT $imceanotes_id IN u.imceanotes THEN u.imceanotes + $imceanotes_id\n",
        "                    ELSE u.imceanotes\n",
        "                END\n",
        "                SET u.parsingError = CASE\n",
        "                    WHEN $parsing_error THEN true\n",
        "                    ELSE u.parsingError\n",
        "                END\n",
        "                WITH u\n",
        "                MATCH (e:Email {{message_id: $message_id}})\n",
        "                MERGE (u)-[r:{rel_type}]->(e)\n",
        "                SET r.incompletePair = $incomplete,\n",
        "                    r.parsingError = $parsing_error\n",
        "            \"\"\", nameRaw=user['raw'], nameNormalized=user['normalized'], \n",
        "                email=associated_email, imceanotes_id=imceanotes_id,\n",
        "                message_id=message_id, incomplete=incomplete, parsing_error=user_parsing_error)\n",
        "        \n",
        "        elif imceanotes_id:\n",
        "            # MERGE on nameRaw + imceanotes_id pair (no email available)\n",
        "            tx.run(f\"\"\"\n",
        "                MERGE (u:User {{nameRaw: $nameRaw, primaryImceanotes: $imceanotes_id}})\n",
        "                SET u.nameNormalized = $nameNormalized\n",
        "                SET u.imceanotes = CASE \n",
        "                    WHEN u.imceanotes IS NULL THEN [$imceanotes_id]\n",
        "                    WHEN NOT $imceanotes_id IN u.imceanotes THEN u.imceanotes + $imceanotes_id\n",
        "                    ELSE u.imceanotes\n",
        "                END\n",
        "                SET u.parsingError = CASE\n",
        "                    WHEN $parsing_error THEN true\n",
        "                    ELSE u.parsingError\n",
        "                END\n",
        "                WITH u\n",
        "                MATCH (e:Email {{message_id: $message_id}})\n",
        "                MERGE (u)-[r:{rel_type}]->(e)\n",
        "                SET r.incompletePair = $incomplete,\n",
        "                    r.parsingError = $parsing_error\n",
        "            \"\"\", nameRaw=user['raw'], nameNormalized=user['normalized'],\n",
        "                imceanotes_id=imceanotes_id, message_id=message_id, \n",
        "                incomplete=incomplete, parsing_error=user_parsing_error)\n",
        "        \n",
        "        else:\n",
        "            # Fall back to nameRaw only (least reliable)\n",
        "            tx.run(f\"\"\"\n",
        "                MERGE (u:User {{nameRaw: $nameRaw}})\n",
        "                SET u.nameNormalized = $nameNormalized\n",
        "                SET u.parsingError = CASE\n",
        "                    WHEN $parsing_error THEN true\n",
        "                    ELSE u.parsingError\n",
        "                END\n",
        "                WITH u\n",
        "                MATCH (e:Email {{message_id: $message_id}})\n",
        "                MERGE (u)-[r:{rel_type}]->(e)\n",
        "                SET r.incompletePair = $incomplete,\n",
        "                    r.parsingError = $parsing_error\n",
        "            \"\"\", nameRaw=user['raw'], nameNormalized=user['normalized'],\n",
        "                message_id=message_id, incomplete=incomplete, parsing_error=user_parsing_error)\n",
        "    \n",
        "    # Link User to Mailbox if we have both\n",
        "    if has_user and has_mailbox:\n",
        "        associated_email = user.get('associated_email')\n",
        "        imceanotes_id = user.get('imceanotes_id')\n",
        "        \n",
        "        if associated_email:\n",
        "            tx.run(\"\"\"\n",
        "                MATCH (u:User {nameRaw: $nameRaw, primaryEmail: $email})\n",
        "                MATCH (m:Mailbox {address: $address})\n",
        "                MERGE (u)-[:USED]->(m)\n",
        "            \"\"\", nameRaw=user['raw'], email=associated_email, address=mailbox['address'])\n",
        "        elif imceanotes_id:\n",
        "            tx.run(\"\"\"\n",
        "                MATCH (u:User {nameRaw: $nameRaw, primaryImceanotes: $imceanotes_id})\n",
        "                MATCH (m:Mailbox {address: $address})\n",
        "                MERGE (u)-[:USED]->(m)\n",
        "            \"\"\", nameRaw=user['raw'], imceanotes_id=imceanotes_id, address=mailbox['address'])\n",
        "        else:\n",
        "            tx.run(\"\"\"\n",
        "                MATCH (u:User {nameRaw: $nameRaw})\n",
        "                WHERE u.primaryEmail IS NULL AND u.primaryImceanotes IS NULL\n",
        "                MATCH (m:Mailbox {address: $address})\n",
        "                MERGE (u)-[:USED]->(m)\n",
        "            \"\"\", nameRaw=user['raw'], address=mailbox['address'])\n",
        "\n",
        "\n",
        "def import_email(tx, email: Dict):\n",
        "    \"\"\"Import a single email with all participants.\"\"\"\n",
        "    if not email or not email.get('message_id'):\n",
        "        return\n",
        "    \n",
        "    # Create Email node\n",
        "    tx.run(\"\"\"\n",
        "        MERGE (e:Email {message_id: $message_id})\n",
        "        SET e.date = $date,\n",
        "            e.subject = $subject,\n",
        "            e.thread = $thread,\n",
        "            e.user = $user,\n",
        "            e.folder = $folder,\n",
        "            e.x_folder = $x_folder\n",
        "    \"\"\", \n",
        "        message_id=email['message_id'],\n",
        "        date=email['date'],\n",
        "        subject=email['subject'],\n",
        "        thread=email.get('thread', ''),\n",
        "        user=email['user'],\n",
        "        folder=email['folder'],\n",
        "        x_folder=email.get('x_folder', '')\n",
        "    )\n",
        "    \n",
        "    # Import sender\n",
        "    import_participant(tx, email.get('from'), email['message_id'], 'SENT')\n",
        "    \n",
        "    # Import recipients\n",
        "    for participant in email.get('to', []):\n",
        "        import_participant(tx, participant, email['message_id'], 'RECEIVED')\n",
        "    \n",
        "    for participant in email.get('cc', []):\n",
        "        import_participant(tx, participant, email['message_id'], 'CC_ON')\n",
        "    \n",
        "    for participant in email.get('bcc', []):\n",
        "        import_participant(tx, participant, email['message_id'], 'BCC_ON')\n",
        "    \n",
        "    # If no recipients at all, infer from folder owner\n",
        "    has_to_recipients = any(p.get('mailbox') or p.get('person') for p in email.get('to', []))\n",
        "\n",
        "    if not has_to_recipients and email.get('user'):\n",
        "        tx.run(\"\"\"\n",
        "            MERGE (m:Mailbox {address: $address})\n",
        "            WITH m\n",
        "            MATCH (e:Email {message_id: $message_id})\n",
        "            MERGE (m)-[r:RECEIVED]->(e)\n",
        "            SET r.inferred = true\n",
        "        \"\"\", address=f\"{email['user']}@enron.com\", message_id=email['message_id'])\n",
        "\n",
        "\n",
        "# Import emails\n",
        "imported = 0\n",
        "errors = 0\n",
        "\n",
        "for file_path in tqdm(files, desc=\"Importing\"):\n",
        "    email = parser.parse(file_path)\n",
        "    if email:\n",
        "        try:\n",
        "            with driver.session() as session:\n",
        "                session.execute_write(import_email, email)\n",
        "            imported += 1\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "    else:\n",
        "        errors += 1\n",
        "\n",
        "print(f\"\\nImported {imported:,} emails ({errors:,} errors)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afdeaaf5",
      "metadata": {},
      "source": [
        "## 7. Verify Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "714d218b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "IMPORT VERIFICATION\n",
            "============================================================\n",
            "\n",
            "Node Counts:\n",
            "  Emails: 10,000\n",
            "  Users: 2,553\n",
            "  Mailboxes: 4,935\n",
            "\n",
            "Relationship Counts:\n",
            "  SENT: 19,131\n",
            "  RECEIVED: 49,148\n",
            "  CC_ON: 6,086\n",
            "\n",
            "Top Senders:\n",
            "  john.arnold@enron.com: 3,491 emails\n",
            "  phillip.allen@enron.com: 2,125 emails\n",
            "  k..allen@enron.com: 297 emails\n",
            "  robert.badeer@enron.com: 193 emails\n",
            "  susan.bailey@enron.com: 143 emails\n",
            "\n",
            "============================================================\n",
            "Expected ranges for FULL dataset:\n",
            "  Emails: ~517,000\n",
            "  Users: ~84,000\n",
            "  Mailboxes: ~99,000\n",
            "  Total Relationships: ~7.5 million\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Verify Import\n",
        "def query(cypher):\n",
        "    with driver.session() as session:\n",
        "        return list(session.run(cypher))\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"IMPORT VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nNode Counts:\")\n",
        "email_count = query('MATCH (e:Email) RETURN count(e) as n')[0]['n']\n",
        "user_count = query('MATCH (u:User) RETURN count(u) as n')[0]['n']\n",
        "mailbox_count = query('MATCH (m:Mailbox) RETURN count(m) as n')[0]['n']\n",
        "\n",
        "print(f\"  Emails: {email_count:,}\")\n",
        "print(f\"  Users: {user_count:,}\")\n",
        "print(f\"  Mailboxes: {mailbox_count:,}\")\n",
        "\n",
        "print(\"\\nRelationship Counts:\")\n",
        "print(f\"  SENT: {query('MATCH ()-[r:SENT]->() RETURN count(r) as n')[0]['n']:,}\")\n",
        "print(f\"  RECEIVED: {query('MATCH ()-[r:RECEIVED]->() RETURN count(r) as n')[0]['n']:,}\")\n",
        "print(f\"  CC_ON: {query('MATCH ()-[r:CC_ON]->() RETURN count(r) as n')[0]['n']:,}\")\n",
        "\n",
        "print(\"\\nTop Senders:\")\n",
        "for record in query(\"MATCH (m:Mailbox)-[:SENT]->(e) RETURN m.address as email, count(e) as sent ORDER BY sent DESC LIMIT 5\"):\n",
        "    print(f\"  {record['email']}: {record['sent']:,} emails\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Expected ranges for FULL dataset:\")\n",
        "print(\"  Emails: ~517,000\")\n",
        "print(\"  Users: ~84,000\") \n",
        "print(\"  Mailboxes: ~99,000\")\n",
        "print(\"  Total Relationships: ~7.5 million\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "314c408a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection closed\n"
          ]
        }
      ],
      "source": [
        "# 8.1 Close Connection\n",
        "driver.close()\n",
        "print(\"Connection closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
